{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb4f46d-bd50-4b76-8744-c12c628aaa83",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# @title Google colab install requirements.\n",
    "%%capture\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if we are in Google Colab\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print(\"Detected Google Colab. Cloning repository...\")\n",
    "    \n",
    "    REPO_URL = \"https://github.com/mwangi-clinton/2026-02-02-knowledge-distillation-for-2d-hpe.git\"\n",
    "    REPO_NAME = \"2026-02-02-knowledge-distillation-for-2d-hpe\"\n",
    "    if not os.path.exists(REPO_NAME):\n",
    "        !git clone $REPO_URL\n",
    "        %cd $REPO_NAME\n",
    "        !pip install -r requirements.txt\n",
    "\n",
    "    sys.path.append(os.path.abspath(\".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3961cd-1d0b-42e8-b020-25d13b81c3c0",
   "metadata": {},
   "source": [
    "# Knowledge Distillation for 2D Human Pose Estimation\n",
    "\n",
    "## 1. Overview\n",
    "Knowledge Distillation (KD) is a deep learning technique where a small, compact model (the **Student**) is trained to reproduce the behavior of a large, complex model (the **Teacher**). The core idea was popularized by Hinton et al. (2015) to transfer \"knowledge\" from a heavy ensemble or high-parameter model to a lightweight one suitable for edge devices.\n",
    "\n",
    "In Human Pose Estimation (HPE):\n",
    "* **Teacher:** High-performance models (e.g., HRNet-W48) that are accurate but computationally expensive.\n",
    "* **Student:** Lightweight models (e.g., SqueezeNet or MobileNet) designed for real-time inference.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Representations in HPE: Gaussian Heatmaps\n",
    "Modern HPE models typically predict **Gaussian Heatmaps** rather than direct $(x, y)$ coordinates. Each keypoint $k$ at position $(x_k, y_k)$ is represented as a probability map where the value at pixel $(i, j)$ is calculated as:\n",
    "\n",
    "$$H_k(i, j) = \\exp\\left( -\\frac{(i - x_k)^2 + (j - y_k)^2}{2\\sigma^2} \\right)$$\n",
    "\n",
    "* **$\\sigma$:** Controls the spread of the Gaussian peak.\n",
    "* **Masks (Visibility):** A binary mask $M$ is used ($1$ if visible, $0$ if not). We multiply the loss by this mask to \"switch off\" learning for unlabelled or invisible joints.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. The Role of Temperature ($T$)\n",
    "The standard Softmax function often produces \"sharp\" distributions where one value nears $1.0$ and others near $0.0$:\n",
    "\n",
    "$$\\sigma(z_i) = \\frac{\\exp(z_i)}{\\sum_j \\exp(z_j)}$$\n",
    "\n",
    "**The Problem:** If a Teacher is too confident (1.0 at peak), it provides no more information than the Ground Truth. We lose the **\"Dark Knowledge\"**—the subtle spatial relationships in the pixels surrounding the peak.\n",
    "\n",
    "**The Solution:** By introducing a Temperature $T > 1$, we \"soften\" the distribution:\n",
    "\n",
    "$$q_i = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)}$$\n",
    "\n",
    "* **High T (> 1):** Flattens the peaks, spreading probability mass to neighboring pixels and revealing the Teacher's spatial uncertainty.\n",
    "* **Low T (= 1):** Returns to the standard sharp Softmax.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Distillation Loss Formulation\n",
    "To distill heatmaps, we treat them as **Spatial Probability Distributions**. We flatten the $H \\times W$ dimensions and apply a Spatial Softmax with temperature.\n",
    "\n",
    "### The KD Loss\n",
    "The Kullback-Leibler (KL) Divergence between the softened student logits $z_s$ and teacher logits $z_t$ is scaled by $T^2$:\n",
    "\n",
    "$$L_{KD} = T^2 \\cdot KL\\left( \\sigma\\left(\\frac{z_s}{T}\\right), \\sigma\\left(\\frac{z_t}{T}\\right) \\right)$$\n",
    "\n",
    "### Total Training Objective\n",
    "The student is trained using a weighted sum of the Ground Truth (GT) loss and the Distillation loss:\n",
    "\n",
    "$$L_{total} = L_{GT} + \\alpha \\cdot L_{KD}$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Implementation: Spatial KL Divergence\n",
    "In practice, the loss is calculated per channel (keypoint) and then averaged.\n",
    "\n",
    "**For a single channel $k$:**\n",
    "$$D_{KL}(P_k || \\hat{P}_k) = \\sum_{x=1}^{W} \\sum_{y=1}^{H} P_k(x, y) \\log \\left( \\frac{P_k(x, y)}{\\hat{P}_k(x, y) + \\epsilon} \\right)$$\n",
    "\n",
    "**Variable Definitions:**\n",
    "* $P_k(x, y)$: The teacher's probability at pixel $(x, y)$.\n",
    "* $\\hat{P}_k(x, y)$: The student's predicted probability at pixel $(x, y)$.\n",
    "* $\\epsilon$: A tiny constant ($10^{-8}$) to prevent $\\log(0)$.\n",
    "\n",
    "**Final Averaged Loss:**\n",
    "$$L_{KL} = \\frac{1}{K} \\sum_{k=1}^{K} D_{KL}(P_k || \\hat{P}_k)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f38ea0b0-82a6-44d4-afe2-83b0c5f7d2f4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Union, Dict\n",
    "import cv2\n",
    "import logging\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pycocotools.coco import COCO\n",
    "from torch import Tensor\n",
    "from typing import Optional, Callable\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from typing import List, Union, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2aa767-4583-4fba-a59f-1a01988b981f",
   "metadata": {
    "id": "ac0f5f71-b364-4a64-90ee-510b04c08d00"
   },
   "source": [
    "#### Student model\n",
    "\n",
    "We design a lightweight model with <a href=\"https://arxiv.org/pdf/1602.07360\">SquuezeNet backbone</a> pretrained to extarct features from the an images. WSo we just remove tthe classification tail of the model and add a custom backbone for generating heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8bc2b59-20b3-4ead-b902-f59b6cd72cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "KeypointOutput = namedtuple('KeypointOutput', ['heatmaps'])\n",
    "class SqueezeNetHPE(nn.Module):\n",
    "    def __init__(self, num_keypoints=17):\n",
    "        super().__init__()\n",
    "        squeezenet = models.squeezenet1_1(weights=models.SqueezeNet1_1_Weights.IMAGENET1K_V1)\n",
    "        self.backbone = squeezenet.features\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv_heatmap = nn.Conv2d(\n",
    "            in_channels=512,\n",
    "            out_channels=num_keypoints,\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.relu(x)\n",
    "        x = F.interpolate(\n",
    "            x,\n",
    "            size=(96, 72), \n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False\n",
    "        )\n",
    "        heatmaps = self.conv_heatmap(x)\n",
    "\n",
    "        return KeypointOutput(heatmaps=heatmaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43a8307-241a-450c-b3a5-25eabecc929d",
   "metadata": {},
   "source": [
    "1.  **Backbone (`self.backbone`)**: We use the feature extractor from `squeezenet1_1` pretrained on ImageNet. <br>\n",
    "2.  **Upsampling**: SqueezeNet significantly downsamples the input (typically by 32x). To generate high-quality heatmaps (which require spatial precision), we use `F.interpolate` to upsample the features to **96x72**.\n",
    "3.  **Heatmap Head (`self.conv_heatmap`)**: Instead of a Fully Connected layer for classification, we use a final **Convolutional Layer** (kernel size 3x3) to produce **17 output channels**, each corresponding to a keypoint heatmap.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f96ebe4-39ed-4918-b5ac-c05eece46a61",
   "metadata": {},
   "source": [
    "#### Teacher model\n",
    "\n",
    "\n",
    "In this tutprial we will use <a href=\"https://github.com/HRNet/HRNet-Human-Pose-Estimation?tab=readme-ov-file\"> HRNet-pose as a teacher model </a>.\n",
    "\n",
    "**HRNet** (High-Resolution Net) maintains high-resolution representations throughout the network. Unlike traditional architectures (ResNet, VGG) that downsample the image and then upsample, HRNet connects high-to-low resolution subnetworks in parallel.\n",
    "\n",
    "\n",
    "**Role as Teacher:**\n",
    "HRNet-pose is a state-of-the-art CNN based model for 2D Pose Estimation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc40104a-fc60-426a-855c-46c1c7f7912f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hrnet_pose.hrnetpose_model import get_pose_net\n",
    "from hrnet_pose.configs import load_configs\n",
    "\n",
    "cfg = load_configs('hrnet_pose/hrnet_w48_model_configs.yaml')\n",
    "teacher_model = get_pose_net(cfg, is_train=False)\n",
    "checkpoint = torch.load('hrnet_pose/hrnet_pose_models/pose_hrnet_w48_384x288.pth', map_location='cpu')\n",
    "teacher_model.load_state_dict(checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07f4e33-86e8-4556-83e8-5f0996fbb64f",
   "metadata": {
    "id": "e07f4e33-86e8-4556-83e8-5f0996fbb64f"
   },
   "source": [
    "# 2. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5e6a66-a521-4def-ae84-c9a39bed1b89",
   "metadata": {},
   "source": [
    "## 2.1 COCO Keypoint Dataset\n",
    "The COCO (Common Objects in Context) dataset is a large-scale object detection, segmentation, and captioning dataset. <br>\n",
    "For Human Pose Estimation, we use the **Keypoints** task, which annotates **17 keypoints** (joints) on human bodies.\n",
    "\n",
    "The 17 Keypoints are:\n",
    "0: Nose, 1: Left Eye, 2: Right Eye, 3: Left Ear, 4: Right Ear, 5: Left Shoulder, 6: Right Shoulder, 7: Left Elbow, 8: Right Elbow, 9: Left Wrist, 10: Right Wrist, 11: Left Hip, 12: Right Hip, 13: Left Knee, 14: Right Knee, 15: Left Ankle, 16: Right Ankle. <br>\n",
    "\n",
    "The keypoints are connected to form a skeleton structure, helping in visualizing pose.\n",
    "\n",
    "![COCO Skeleton](images/keypoints-skeleton.png) ![COCO Examples](images/keypoints-examples.png)\n",
    "\n",
    "*Example of COCO Keypoint Annotations (Source: COCO Dataset)*\n",
    "\n",
    "Read more about COCO dataset <a href=\"https://cocodataset.org/#keypoints-2017\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed95543-736a-435e-8e32-87e427398222",
   "metadata": {},
   "source": [
    "### 2.1.1 Get data using fiftyone library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc4fb73-1189-4304-8ed0-0c31ff32d748",
   "metadata": {},
   "source": [
    "[FiftyOne](https://voxel51.com/fiftyone/) is an open-source toolset by Voxel51 designed for building high-quality datasets and computer vision models. It acts as a visual interface for your datasets, allowing you to:\n",
    "\n",
    "- **Visualize** raw images and their annotations (bounding boxes, keypoints, etc.) instantly.\n",
    "- **Query** specific samples (e.g., \"show me all images with > 10 people\").\n",
    "- **Debug** model predictions by overlaying them on ground truth.\n",
    "- **Manage** data splits and export to various formats (COCO, YOLO, TFRecord)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35f3f3ef-3e88-4369-8cf7-98c591739ac2",
   "metadata": {
    "id": "35f3f3ef-3e88-4369-8cf7-98c591739ac2",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "f22f7827-0ae6-497b-b83f-76b1b58372a8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clinton-mwangi/Desktop/projects/2d-human-pose-estimation-using-kd/kd_hpe_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b855961-7fae-4161-b88e-25dcddc26301",
   "metadata": {
    "collapsed": true,
    "id": "9b855961-7fae-4161-b88e-25dcddc26301",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "outputId": "9e82d832-1bad-4b63-fd76-8cfc2f7c414f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'train' to '/home/clinton-mwangi/fiftyone/coco-2017/train' if necessary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 17:58:59,125 - INFO - Downloading split 'train' to '/home/clinton-mwangi/fiftyone/coco-2017/train' if necessary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading annotations to '/home/clinton-mwangi/fiftyone/coco-2017/tmp-download/annotations_trainval2017.zip'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 17:58:59,127 - INFO - Downloading annotations to '/home/clinton-mwangi/fiftyone/coco-2017/tmp-download/annotations_trainval2017.zip'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |██████|    1.9Gb/1.9Gb [12.7m elapsed, 0s remaining, 6.9Mb/s]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 18:11:39,733 - INFO -  100% |██████|    1.9Gb/1.9Gb [12.7m elapsed, 0s remaining, 6.9Mb/s]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting annotations to '/home/clinton-mwangi/fiftyone/coco-2017/raw/instances_train2017.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 18:11:39,734 - INFO - Extracting annotations to '/home/clinton-mwangi/fiftyone/coco-2017/raw/instances_train2017.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 3000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 18:11:58,984 - INFO - Downloading 3000 images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |████████████████| 3000/3000 [14.4m elapsed, 0s remaining, 3.7 images/s]      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 18:26:22,450 - INFO -  100% |████████████████| 3000/3000 [14.4m elapsed, 0s remaining, 3.7 images/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing annotations for 3000 downloaded samples to '/home/clinton-mwangi/fiftyone/coco-2017/train/labels.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 18:26:22,467 - INFO - Writing annotations for 3000 downloaded samples to '/home/clinton-mwangi/fiftyone/coco-2017/train/labels.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info written to '/home/clinton-mwangi/fiftyone/coco-2017/info.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 18:26:25,342 - INFO - Dataset info written to '/home/clinton-mwangi/fiftyone/coco-2017/info.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 'coco-2017' split 'train'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 18:26:25,396 - INFO - Loading 'coco-2017' split 'train'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 3000/3000 [1.1s elapsed, 0s remaining, 2.8K samples/s]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 18:26:27,051 - INFO -  100% |███████████████| 3000/3000 [1.1s elapsed, 0s remaining, 2.8K samples/s]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'coco-2017-train-3000' created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 18:26:27,060 - INFO - Dataset 'coco-2017-train-3000' created\n"
     ]
    }
   ],
   "source": [
    "train_data = foz.load_zoo_dataset(\"coco-2017\", split='train',max_samples=3000,  label_types=[\"keypoints\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc73542c-6b56-4aec-ac06-2faeddb3c058",
   "metadata": {
    "collapsed": true,
    "id": "dc73542c-6b56-4aec-ac06-2faeddb3c058",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "fbbf127a-6813-4dbd-9915-2fd02fba6b15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'validation' to '/home/clinton-mwangi/fiftyone/coco-2017/validation' if necessary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 18:26:27,091 - INFO - Downloading split 'validation' to '/home/clinton-mwangi/fiftyone/coco-2017/validation' if necessary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found annotations at '/home/clinton-mwangi/fiftyone/coco-2017/raw/instances_val2017.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 18:26:27,093 - INFO - Found annotations at '/home/clinton-mwangi/fiftyone/coco-2017/raw/instances_val2017.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 18:26:27,911 - INFO - Downloading 1000 images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |████████████████| 1000/1000 [4.4m elapsed, 0s remaining, 5.0 images/s]      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 18:30:54,124 - INFO -  100% |████████████████| 1000/1000 [4.4m elapsed, 0s remaining, 5.0 images/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing annotations for 1000 downloaded samples to '/home/clinton-mwangi/fiftyone/coco-2017/validation/labels.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 18:30:54,142 - INFO - Writing annotations for 1000 downloaded samples to '/home/clinton-mwangi/fiftyone/coco-2017/validation/labels.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info written to '/home/clinton-mwangi/fiftyone/coco-2017/info.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 18:30:54,361 - INFO - Dataset info written to '/home/clinton-mwangi/fiftyone/coco-2017/info.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing dataset 'coco-2017-validation-1000'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 18:30:54,364 - INFO - Loading existing dataset 'coco-2017-validation-1000'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    }
   ],
   "source": [
    "val_data = foz.load_zoo_dataset(\"coco-2017\", split=\"validation\",  max_samples=1000, label_types=[\"keypoints\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e44a357-7a11-4071-a95d-88821bcfa4a8",
   "metadata": {
    "collapsed": true,
    "id": "2e44a357-7a11-4071-a95d-88821bcfa4a8",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "4cce8813-f18e-4753-96a9-2d4db191daae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name:        coco-2017-train-3000\n",
       "Media type:  image\n",
       "Num samples: 3000\n",
       "Persistent:  False\n",
       "Tags:        []\n",
       "Sample fields:\n",
       "    id:               fiftyone.core.fields.ObjectIdField\n",
       "    filepath:         fiftyone.core.fields.StringField\n",
       "    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
       "    created_at:       fiftyone.core.fields.DateTimeField\n",
       "    last_modified_at: fiftyone.core.fields.DateTimeField\n",
       "    ground_truth:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Keypoints)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd79bb20-0339-452d-a5b1-3cccd18ce6cf",
   "metadata": {
    "id": "bd79bb20-0339-452d-a5b1-3cccd18ce6cf",
    "outputId": "9b466a28-d275-448a-edf6-2573908bfe9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session launched. Run `session.show()` to open the App in a cell output.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 18:37:10,064 - INFO - Session launched. Run `session.show()` to open the App in a cell output.\n"
     ]
    }
   ],
   "source": [
    "session = fo.launch_app(val_data, auto=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9077f055-d120-43ee-9dab-28bc2d254225",
   "metadata": {
    "id": "045c85fd-69ba-4152-b87b-c54b96c559cb",
    "outputId": "8721e225-5afa-4c8a-e59f-76718515bbe7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?notebook=True&subscription=df40de7f-c614-40da-86d8-d4bf73336bef\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7b5ac1f43170>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "session.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177333ad-6c81-4fcb-bf15-c7e5272240c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.1.2. Get data by direct download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ae12c2-ea69-4587-9389-3f57af6dd28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to store the data\n",
    "DATA_DIR=\"/path/to/store/data\"\n",
    "\n",
    "!mkdir -p $DATA_DIR\n",
    "\n",
    "!wget -c http://images.cocodataset.org/annotations/annotations_trainval2017.zip -P $DATA_DIR\n",
    "!wget -c http://images.cocodataset.org/zips/val2017.zip -P $DATA_DIR\n",
    "!wget -c http://images.cocodataset.org/zips/train2017.zip -P $DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc524cf-358a-453b-bb1b-34f21c469a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the files\n",
    "!unzip -q $DATA_DIR/annotations_trainval2017.zip -d $DATA_DIR\n",
    "!unzip -q $DATA_DIR/val2017.zip -d $DATA_DIR\n",
    "!unzip -q $DATA_DIR/train2017.zip -d $DATA_DIR\n",
    "\n",
    "# Cleanup zip files\n",
    "!rm -rf $DATA_DIR/*.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeaef20-70c6-4bb2-a955-1cad726b23ca",
   "metadata": {
    "id": "dbeaef20-70c6-4bb2-a955-1cad726b23ca"
   },
   "source": [
    "## 2.2 Data Prepocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9d733d-8a11-4f40-95ae-0d14343312be",
   "metadata": {
    "id": "ee9d733d-8a11-4f40-95ae-0d14343312be"
   },
   "source": [
    "We will follow the following data preprocessing that are unique to human pose estimation tasks based on paper: <a href=\"https://arxiv.org/abs/1911.07524\"> The devils is in the details </a>.\n",
    "1.  **Filtering**: We first check if an image actually contains a person with valid annotations. \n",
    "2.  **Cropping**: Once we identify a valid image, we locate the person using their bounding box and **crop** the image to center on them. \n",
    "3.  **Coordinate Transformation**: We transform the original keypoint annotations from the full image space into our new **cropped coordinate space**.\n",
    "4.  **Ground Truth Generation**: Finally, we take these transformed keypoints and generate **Gaussian Heatmaps**, which serve as the training targets for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7aa3f3b-708f-45ae-8c1f-3056e1937957",
   "metadata": {
    "id": "b7aa3f3b-708f-45ae-8c1f-3056e1937957",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, target_size=(288, 384), heatmap_size=(72, 96), sigma=2.0):\n",
    "        self.target_size = target_size\n",
    "        self.heatmap_size = heatmap_size\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        # Pre-compute the coordinate grid for heatmap generation\n",
    "        W, H = self.heatmap_size\n",
    "        self.yy, self.xx = np.meshgrid(np.arange(H), np.arange(W), indexing='ij')\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_keypoints_and_visibility(keypoints: List[float]):\n",
    "        try:\n",
    "            kps_array = np.array(keypoints, dtype=np.float32).reshape(-1, 3)\n",
    "            coords = kps_array[:, :2]\n",
    "            visibility = kps_array[:, 2]\n",
    "            return coords, visibility.astype(np.int32)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting keypoints: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def process_image_and_keypoints(\n",
    "        self,\n",
    "        image: np.ndarray,\n",
    "        keypoints: np.ndarray,\n",
    "        bbox: Union[List[int], Tuple[int, int, int, int]],\n",
    "        angle: float = 0,\n",
    "        flip: bool = False\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Sequential process of proocessing the input image\n",
    "        \"\"\"\n",
    "        try:\n",
    "            x1, y1, w, h = np.round(bbox).astype(int)\n",
    "            img_h, img_w = image.shape[:2]\n",
    "\n",
    "            # Step 1: Safe Crop\n",
    "            cx1, cy1 = max(0, x1), max(0, y1) #Why do we do this?\n",
    "            cx2, cy2 = min(img_w, x1 + w), min(img_h, y1 + h)\n",
    "            image = image[cy1:cy2, cx1:cx2]\n",
    "            crop_h, crop_w = image.shape[:2]\n",
    "\n",
    "            # Step 2: Prepare Keypoints\n",
    "            kps = keypoints.copy().astype(np.float32)\n",
    "            zero_mask = np.all(kps == 0, axis=1)\n",
    "            kps -= [cx1, cy1]\n",
    "\n",
    "            # Step 3: Boundary Masking (Vectorized)\n",
    "            invalid_mask = (kps[:, 0] < 0) | (kps[:, 1] < 0) | (kps[:, 0] >= crop_w) | (kps[:, 1] >= crop_h)\n",
    "\n",
    "            # Step 4: Resize (High Speed via OpenCV)\n",
    "            image = cv2.resize(image, self.target_size, interpolation=cv2.INTER_LINEAR)\n",
    "            kps *= [self.target_size[0] / crop_w, self.target_size[1] / crop_h]\n",
    "\n",
    "            # Step 5: Rotation\n",
    "            if angle != 0:\n",
    "                center = (self.target_size[0] / 2, self.target_size[1] / 2)\n",
    "                M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "                image = cv2.warpAffine(image, M, self.target_size)\n",
    "                ones = np.ones((kps.shape[0], 1))\n",
    "                kps_homo = np.hstack([kps, ones])\n",
    "                kps = kps_homo @ M.T\n",
    "\n",
    "            # Step 6: Horizontal Flip\n",
    "            if flip:\n",
    "                image = cv2.flip(image, 1)\n",
    "                kps[:, 0] = self.target_size[0] - kps[:, 0]\n",
    "\n",
    "            # Step 7: Restore invalid points to zero\n",
    "            kps[zero_mask | invalid_mask] = 0\n",
    "\n",
    "            return image, kps\n",
    "        except Exception as e:\n",
    "            print(f\"Error in processing: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def generate_heatmaps(\n",
    "        self,\n",
    "        keypoints: np.ndarray,\n",
    "        keypoints_visible: np.ndarray\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Highly optimized Gaussian Heatmap generation using pre-computed grids.\n",
    "        \"\"\"\n",
    "        W_hm, H_hm = self.heatmap_size\n",
    "        W_img, H_img = self.target_size\n",
    "        scale_x = W_hm / W_img\n",
    "        scale_y = H_hm / H_img\n",
    "        kps_hm = keypoints * [scale_x, scale_y]\n",
    "\n",
    "        mask = (keypoints_visible >= 0.5) & \\\n",
    "               (kps_hm[:, 0] >= 0) & (kps_hm[:, 0] < W_hm) & \\\n",
    "               (kps_hm[:, 1] >= 0) & (kps_hm[:, 1] < H_hm)\n",
    " \n",
    "        num_kps = kps_hm.shape[0]\n",
    "        heatmaps = np.zeros((num_kps, H_hm, W_hm), dtype=np.float32)\n",
    "        for i in range(num_kps):\n",
    "            if not mask[i]:\n",
    "                continue\n",
    "                \n",
    "            mu_x, mu_y = kps_hm[i]\n",
    "            dist_sq = (self.xx - mu_x) ** 2 + (self.yy - mu_y) ** 2\n",
    "            heatmaps[i] = np.exp(-dist_sq / (2 * self.sigma**2))\n",
    "\n",
    "        return torch.from_numpy(heatmaps), torch.from_numpy(mask.astype(np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9ae3dd-2af1-487a-b2b7-47273275328d",
   "metadata": {
    "id": "ac9ae3dd-2af1-487a-b2b7-47273275328d"
   },
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea9cdf37-efbb-4436-b711-0ddeffd4b8cf",
   "metadata": {
    "id": "ea9cdf37-efbb-4436-b711-0ddeffd4b8cf",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def get_downloaded_image_ids(root_dir: str, coco: COCO) -> List[int]:\n",
    "    on_disk = set(os.listdir(root_dir))\n",
    "    all_img_info = coco.loadImgs(coco.getImgIds())\n",
    "    existing_ids = [info['id'] for info in all_img_info if info['file_name'] in on_disk]\n",
    "    return existing_ids\n",
    "\n",
    "class CocoKeypoints(Dataset):\n",
    "    def __init__(self, root: str, annFile: str, target_size=(288, 384), heatmap_size=(72, 96), sigma=2.0) -> None:\n",
    "        self.root = root\n",
    "        self.coco = COCO(annFile)\n",
    "        self.processor = DataProcessor(\n",
    "            target_size=target_size, \n",
    "            heatmap_size=heatmap_size, \n",
    "            sigma=sigma\n",
    "        )\n",
    "        \n",
    "        on_disk = set(os.listdir(root))\n",
    "        self.samples = []\n",
    "        img_ids = self.coco.getImgIds()\n",
    "        all_imgs = self.coco.loadImgs(img_ids)\n",
    "        img_id_to_file = {img['id']: img['file_name'] for img in all_imgs if img['file_name'] in on_disk}\n",
    "\n",
    "        for img_id in img_id_to_file:\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "            for ann in anns:\n",
    "                if ann.get(\"num_keypoints\", 0) > 0:\n",
    "                    self.samples.append({\n",
    "                        \"file_name\": img_id_to_file[img_id],\n",
    "                        \"keypoints\": ann[\"keypoints\"],\n",
    "                        \"bbox\": ann[\"bbox\"]\n",
    "                    })\n",
    "\n",
    "        logger.info(f\"Initialized {len(self.samples)} samples.\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        sample = self.samples[index]\n",
    "        img_path = os.path.join(self.root, sample[\"file_name\"])\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            return torch.zeros((3, *self.processor.target_size)), torch.zeros((17, *self.processor.heatmap_size)), \n",
    "            torch.zeros(17), torch.zeros((17, 2))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        coords, visibility = self.processor.extract_keypoints_and_visibility(sample[\"keypoints\"])\n",
    "        processed_img_np, processed_kps = self.processor.process_image_and_keypoints(\n",
    "            image, \n",
    "            coords, \n",
    "            sample[\"bbox\"]\n",
    "        )\n",
    "        heatmaps, masks = self.processor.generate_heatmaps(processed_kps, visibility)\n",
    "        img_tensor = torch.from_numpy(processed_img_np.copy()).permute(2, 0, 1).float().div(255.0)\n",
    "        coords_tensors = torch.from_numpy(processed_kps.copy()) \n",
    "        return img_tensor, heatmaps, masks, coords_tensors\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "def get_coco_dataloaders(root_train, ann_train, root_val, ann_val, batch_size=32, num_workers=8):\n",
    "    train_dataset = CocoKeypoints(root_train, ann_train)\n",
    "    val_dataset = CocoKeypoints(root_val, ann_val)    \n",
    "    loader_args = dict(\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True, \n",
    "        collate_fn=collate_fn,\n",
    "        persistent_workers=True \n",
    "    )\n",
    "    return (\n",
    "        DataLoader(train_dataset, shuffle=True, **loader_args),\n",
    "        DataLoader(val_dataset, shuffle=False, **loader_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69b956cc-0790-4445-bf03-dbecea82fdea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/clinton-mwangi/fiftyone'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zoo_dir = fo.config.dataset_zoo_dir\n",
    "zoo_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ab6c16b-98fc-4d76-8341-f026eeab3390",
   "metadata": {
    "id": "6ab6c16b-98fc-4d76-8341-f026eeab3390",
    "outputId": "dedb4965-6359-4ba8-adae-76b71f91bc18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=4.91s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 18:41:31,248 - INFO - Initialized 3854 samples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 18:41:31,518 - INFO - Initialized 1275 samples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.22s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "#Fiftyone downloads the data in your home directory by default\n",
    "\n",
    "coco_root = os.path.join(fo.config.dataset_zoo_dir, \"coco-2017\")\n",
    "root_train = os.path.join(coco_root, \"train\", \"data\")\n",
    "root_val = os.path.join(coco_root, \"validation\", \"data\")\n",
    "\n",
    "ann_train = os.path.join(coco_root, \"raw\", \"person_keypoints_train2017.json\")\n",
    "ann_val = os.path.join(coco_root, \"raw\", \"person_keypoints_val2017.json\")\n",
    "\n",
    "train_loader, val_loader = get_coco_dataloaders(\n",
    "    root_train=root_train,\n",
    "    ann_train=ann_train,\n",
    "    root_val=root_val,\n",
    "    ann_val=ann_val,\n",
    "    batch_size=30,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62702d8c-33db-4d88-89af-806cde700003",
   "metadata": {},
   "source": [
    "# 3. Training and distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586def00-6565-4c0c-8d7f-5e84a62a0b99",
   "metadata": {},
   "source": [
    "## 3.1 Baseline Student Training\n",
    "\n",
    "We start by training the **Student Model (SqueezeNet)** purely on the Ground Truth labels, **without any distillation**. on the sample data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b606189-a609-4d68-858f-b9a81cbd3c10",
   "metadata": {
    "id": "5b606189-a609-4d68-858f-b9a81cbd3c10"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "student_model = SqueezeNetHPE(num_keypoints=17).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11c66bc-a1f5-42cd-bd03-779ddd4f4cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineTrainer:\n",
    "    def __init__(self, model, device, \n",
    "                 checkpoint_dir='checkpoints_baseline',\n",
    "                 lr=1e-3, num_epochs=10):\n",
    "        self.device = device\n",
    "        self.model = model.to(device)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.best_val_loss = float('inf')\n",
    "\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        self.criterion = nn.KLDivLoss(reduction='none').to(device)        \n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=1e-4)        \n",
    "        \n",
    "        # Scheduler: 5 epoch warmup + Cosine Decay\n",
    "        lambda_lr = lambda e: (e + 1) / 5 if e < 5 else 0.5 * (1 + math.cos(math.pi * (e - 5) / (max(1, num_epochs - 5))))\n",
    "        self.scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=lambda_lr)\n",
    "        self.scaler = GradScaler()\n",
    "\n",
    "    def train(self, train_loader, val_loader):\n",
    "        print(f\"--- Starting Training ---\")\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.model.train()            \n",
    "            running_loss = 0.0\n",
    "            pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{self.num_epochs}')            \n",
    "            \n",
    "            for imgs, targets, masks, _ in pbar:\n",
    "                imgs = imgs.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                masks = masks.to(self.device)\n",
    "                \n",
    "                self.optimizer.zero_grad(set_to_none=True)                \n",
    "                with autocast():\n",
    "                    outputs = self.model(imgs)\n",
    "                    preds = outputs.heatmaps if hasattr(outputs, 'heatmaps') else outputs\n",
    "                    B, K, H, W = preds.shape\n",
    "\n",
    "                    flat_preds = preds.view(B, K, -1)\n",
    "                    flat_targets = targets.view(B, K, -1)\n",
    "\n",
    "                    log_probs = F.log_softmax(flat_preds, dim=-1)\n",
    "                    target_probs = flat_targets / (flat_targets.sum(dim=-1, keepdim=True) + 1e-8)\n",
    "\n",
    "                    loss_elementwise = self.criterion(log_probs, target_probs)\n",
    "                    loss_per_keypoint = loss_elementwise.sum(dim=-1)\n",
    "\n",
    "                    masked_loss = loss_per_keypoint * masks\n",
    "                    loss = masked_loss.sum() / (masks.sum() + 1e-8)\n",
    "\n",
    "                self.scaler.scale(loss).backward()\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "            self.scheduler.step()\n",
    "            if (epoch + 1) >= 5:\n",
    "                val_loss = self.validate(val_loader)\n",
    "                print(f\"Epoch {epoch+1} | Val Loss: {val_loss:.6f}\")\n",
    "                \n",
    "                if val_loss < self.best_val_loss:\n",
    "                    self.best_val_loss = val_loss\n",
    "                    save_path = os.path.join(self.checkpoint_dir, 'best_baseline_kl.pth')\n",
    "                    torch.save(self.model.state_dict(), save_path)\n",
    "                    print(f\"New best model saved to {save_path}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1} completed.\")\n",
    "\n",
    "    def validate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for imgs, targets, masks, _ in val_loader:\n",
    "                imgs = imgs.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                masks = masks.to(self.device)\n",
    "                \n",
    "                outputs = self.model(imgs)\n",
    "                preds = outputs.heatmaps if hasattr(outputs, 'heatmaps') else outputs\n",
    "                \n",
    "                B, K, H, W = preds.shape\n",
    "                flat_preds = preds.view(B, K, -1)\n",
    "                flat_targets = targets.view(B, K, -1)\n",
    "                \n",
    "                log_probs = F.log_softmax(flat_preds, dim=-1)\n",
    "                target_probs = flat_targets / (flat_targets.sum(dim=-1, keepdim=True) + 1e-8)\n",
    "                \n",
    "                loss_elementwise = self.criterion(log_probs, target_probs)\n",
    "                loss_per_keypoint = loss_elementwise.sum(dim=-1)\n",
    "                \n",
    "                masked_loss = loss_per_keypoint * masks\n",
    "                loss = masked_loss.sum() / (masks.sum() + 1e-8)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "        return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39028ab1-18c3-410e-9ac3-a3b4b5d6d0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = BaselineTrainer(\n",
    "    model=student_model,           \n",
    "    device=device,\n",
    "    checkpoint_dir='experiments/run_01', \n",
    "    lr=1e-3,\n",
    "    num_epochs=50              \n",
    ")\n",
    "trainer.train(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5453a0eb-7ceb-44be-90d4-267de0bcd2a6",
   "metadata": {},
   "source": [
    "## 3.2. Logits-based Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6beadef-0f70-4878-8ef1-a6e80634e1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import math\n",
    "\n",
    "class LogitDistillationTrainer:\n",
    "    def __init__(self, student_model, teacher_model, device, \n",
    "                 checkpoint_dir='checkpoints_distillation',\n",
    "                 lr=1e-3, num_epochs=10, temp=4.0, alpha_logit=1.0):\n",
    "        self.device = device\n",
    "        self.student = student_model.to(device)\n",
    "        self.teacher = teacher_model.to(device).eval() \n",
    "        self.num_epochs = num_epochs\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        \n",
    "        # Distillation Hyperparameters\n",
    "        self.temp = temp\n",
    "        self.alpha_logit = alpha_logit\n",
    "        self.best_val_loss = float('inf')\n",
    "\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        self.kl_criterion = nn.KLDivLoss(reduction='none').to(device)\n",
    "        \n",
    "        self.optimizer = optim.AdamW(self.student.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        \n",
    "        lambda_lr = lambda e: (e + 1) / 5 if e < 5 else 0.5 * (1 + math.cos(math.pi * (e - 5) / (max(1, num_epochs - 5))))\n",
    "        self.scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=lambda_lr)\n",
    "        self.scaler = GradScaler()\n",
    "\n",
    "    def spatial_distillation_loss(self, s_logits, t_logits, masks):\n",
    "        \"\"\"\n",
    "        Implements the Spatial Softmax KL Divergence theory.\n",
    "        \"\"\"\n",
    "        B, K, H, W = s_logits.shape\n",
    "        \n",
    "        # 1. Flatten spatial dimensions\n",
    "        s_logits_flat = s_logits.view(B, K, -1)\n",
    "        t_logits_flat = t_logits.view(B, K, -1)\n",
    "        \n",
    "        # 2. Soften and compute distributions\n",
    "        s_log_prob = F.log_softmax(s_logits_flat / self.temp, dim=-1)\n",
    "        t_prob = F.softmax(t_logits_flat / self.temp, dim=-1)\n",
    "        \n",
    "        # 3. Compute KL Divergence scaled by T^2\n",
    "        kl_loss = self.kl_criterion(s_log_prob, t_prob).sum(dim=-1) \n",
    "        \n",
    "        # 4. Mask and average over valid keypoints\n",
    "        masked_kl = kl_loss * masks\n",
    "        return (masked_kl.sum() / (masks.sum() + 1e-8)) * (self.temp ** 2)\n",
    "\n",
    "    def train(self, train_loader, val_loader):\n",
    "        print(f\"--- Starting distillation training (T={self.temp}) ---\")\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.student.train()\n",
    "            running_loss = 0.0\n",
    "            pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{self.num_epochs}')\n",
    "            \n",
    "            for imgs, targets, masks, _ in pbar:\n",
    "                imgs, targets, masks = imgs.to(self.device), targets.to(self.device), masks.to(self.device)\n",
    "                \n",
    "                self.optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                with autocast():\n",
    "                    s_out = self.student(imgs)\n",
    "                    s_hms = s_out.heatmaps \n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        t_out = self.teacher(imgs)\n",
    "                        t_hms = t_out.heatmaps if hasattr(t_out, 'heatmaps') else t_out\n",
    "\n",
    "                    B, K, _, _ = s_hms.shape\n",
    "                    s_log_probs_gt = F.log_softmax(s_hms.view(B, K, -1), dim=-1)\n",
    "                    t_probs_gt = targets.view(B, K, -1) / (targets.view(B, K, -1).sum(dim=-1, keepdim=True) + 1e-8)\n",
    "                    \n",
    "                    loss_gt = (self.kl_criterion(s_log_probs_gt, t_probs_gt).sum(dim=-1) * masks).sum() / (masks.sum() + 1e-8)\n",
    "\n",
    "                    loss_kd = self.spatial_distillation_loss(s_hms, t_hms, masks)\n",
    "\n",
    "                    total_loss = loss_gt + (self.alpha_logit * loss_kd)\n",
    "\n",
    "                self.scaler.scale(total_loss).backward()\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "                \n",
    "                running_loss += total_loss.item()\n",
    "                pbar.set_postfix({'loss': f\"{total_loss.item():.4f}\", 'kd': f\"{loss_kd.item():.4f}\"})\n",
    "\n",
    "            self.scheduler.step()\n",
    "\n",
    "            if (epoch + 1) >= 5:\n",
    "                val_loss = self.validate(val_loader)\n",
    "                print(f\"Epoch {epoch+1} | Val Loss: {val_loss:.6f}\")\n",
    "                \n",
    "                if val_loss < self.best_val_loss:\n",
    "                    self.best_val_loss = val_loss\n",
    "                    save_path = os.path.join(self.checkpoint_dir, f'best_logit_kd_T{self.temp}.pth')\n",
    "                    torch.save(self.student.state_dict(), save_path)\n",
    "                    print(f\"New best model saved to {save_path}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1} finished. Validation starts at epoch 5.\")\n",
    "\n",
    "    def validate(self, val_loader):\n",
    "        self.student.eval()\n",
    "        total_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for imgs, targets, masks, _ in val_loader:\n",
    "                imgs, targets, masks = imgs.to(self.device), targets.to(self.device), masks.to(self.device)\n",
    "                \n",
    "                outputs = self.student(imgs)\n",
    "                preds = outputs.heatmaps if hasattr(outputs, 'heatmaps') else outputs\n",
    "                \n",
    "                B, K = preds.shape[0], preds.shape[1]\n",
    "                log_probs = F.log_softmax(preds.view(B, K, -1), dim=-1)\n",
    "                target_probs = targets.view(B, K, -1) / (targets.view(B, K, -1).sum(dim=-1, keepdim=True) + 1e-8)\n",
    "                \n",
    "                loss = (self.kl_criterion(log_probs, target_probs).sum(dim=-1) * masks).sum() / (masks.sum() + 1e-8)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "        return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logit_distillation_invocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model_logits = SqueezeNetHPE(num_keypoints=17).to(device)\n",
    "\n",
    "trainer_logits = LogitDistillationTrainer(\n",
    "    student_model=student_model_logits, \n",
    "    teacher_model=teacher_model, \n",
    "    device=device, \n",
    "    checkpoint_dir='checkpoints_logits_kd',\n",
    "    lr=1e-3, \n",
    "    num_epochs=50, \n",
    "    temp=4.0, \n",
    "    alpha_logit=1.0\n",
    ")\n",
    "trainer_logits.train(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3071fc45-e417-47b5-8f3e-2e4a4c9a28a6",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "At this part we will do a simple evaluatin of the models.\n",
    "\n",
    "1.  **Qualitative Accuracy (Visual)** (Using FiftyOne to visualize model predictions vs Ground Truth.)\n",
    "2.  **Model Complexity** (FLOPs and Parameters.)\n",
    "3.  **Coco performance metrics** - Average Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee384f5-4b18-49bf-bb66-54187867a172",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self, device: torch.device):\n",
    "        self.device = device\n",
    "\n",
    "    def count_trainable_parameters(self, model: nn.Module, model_name: str = \"Model\") -> int:\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        total_size_mb = (sum(p.numel() for p in model.parameters()) * 4) / (1024**2)\n",
    "        print(f\"--- {model_name} Profile ---\")\n",
    "        print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "        print(f\"Model Size: {total_size_mb:.2f} MB\\n\")\n",
    "        return trainable_params\n",
    "\n",
    "    def compute_model_complexity(self, model: nn.Module, input_shape: Tuple[int, int, int], name_of_model: str = \"Model\"):\n",
    "        try:\n",
    "            from ptflops import get_model_complexity_info\n",
    "            silent_buffer = io.StringIO()\n",
    "            model.eval()\n",
    "            macs, _ = get_model_complexity_info(model, input_shape, as_strings=False, \n",
    "                                               print_per_layer_stat=False, verbose=False, ost=silent_buffer)\n",
    "            results = {\"GMACs\": macs / 1e9, \"GFLOPs\": (2 * macs) / 1e9}\n",
    "            print(f\"--- {name_of_model} Complexity ---\")\n",
    "            print(f\"MACs: {results['GMACs']:.3f} G | FLOPs: {results['GFLOPs']:.3f} G\\n\")\n",
    "            return results\n",
    "        except ImportError:\n",
    "            return None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def add_predictions_to_fiftyone(self, dataset, model, num_samples=20, batch_size=8, \n",
    "                                   input_size=(288, 384), heatmap_size=(72, 96), field_name=\"predictions\"):\n",
    "        model.eval().to(self.device)\n",
    "        view = dataset.take(num_samples)\n",
    "        samples_list = list(view)\n",
    "        for i in tqdm(range(0, num_samples, batch_size), desc=f\"Predicting {field_name}\"):\n",
    "            batch_samples = samples_list[i : i + batch_size]\n",
    "            batch_imgs = [cv2.resize(cv2.cvtColor(cv2.imread(s.filepath), cv2.COLOR_BGR2RGB), input_size) for s in batch_samples]\n",
    "            inputs = torch.from_numpy(np.stack(batch_imgs)).permute(0, 3, 1, 2).float().to(self.device) / 255.0\n",
    "            outputs = model(inputs)\n",
    "            hms = outputs.heatmaps if hasattr(outputs, \"heatmaps\") else outputs\n",
    "            preds, maxvals = self._get_max_preds_vectorized(hms.cpu().numpy())\n",
    "            for j, sample in enumerate(batch_samples):\n",
    "                norm_points = (preds[j] / [heatmap_size[0], heatmap_size[1]]).clip(0, 1).tolist()\n",
    "                sample[field_name] = fo.Keypoints(keypoints=[fo.Keypoint(points=norm_points, confidence=maxvals[j].flatten().tolist())])\n",
    "                sample.save()\n",
    "        return view\n",
    "\n",
    "    def _get_max_preds_vectorized(self, batch_heatmaps):\n",
    "        B, K, H, W = batch_heatmaps.shape\n",
    "        heatmaps_reshaped = batch_heatmaps.reshape((B, K, -1))\n",
    "        idx = np.argmax(heatmaps_reshaped, axis=2).reshape((B, K, 1))\n",
    "        maxvals = np.amax(heatmaps_reshaped, axis=2).reshape((B, K, 1))\n",
    "        preds = np.zeros((B, K, 2), dtype=np.float32)\n",
    "        preds[:, :, 0], preds[:, :, 1] = idx[:, :, 0] % W, idx[:, :, 0] // W\n",
    "        return preds, maxvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373d7ff4-b109-42bf-8aa9-5768c0b171dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding skeleton for visualization\n",
    "skeleton = fo.KeypointSkeleton(\n",
    "    labels=[\n",
    "        \"nose\", \"l_eye\", \"r_eye\", \"l_ear\", \"r_ear\", \n",
    "        \"l_shoulder\", \"r_shoulder\", \"l_elbow\", \"r_elbow\", \n",
    "        \"l_wrist\", \"r_wrist\", \"l_hip\", \"r_hip\", \n",
    "        \"l_knee\", \"r_knee\", \"l_ankle\", \"r_ankle\"\n",
    "    ],\n",
    "    edges=[\n",
    "        [0, 1], [0, 2], [1, 3], [2, 4], # Head\n",
    "        [5, 6], [5, 7], [7, 9], [6, 8], [8, 10], # Arms\n",
    "        [5, 11], [6, 12], [11, 12], # Torso\n",
    "        [11, 13], [13, 15], [12, 14], [14, 16] # Legs\n",
    "    ]\n",
    ")\n",
    "val_data.default_skeleton = skeleton\n",
    "val_data.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554ba013-c39d-493c-a4ea-932ebd03dfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ModelEvaluator(device=device)\n",
    "\n",
    "\n",
    "evaluator.count_trainable_parameters(teacher_model, \"Teacher (HRNet)\")\n",
    "evaluator.compute_model_complexity(teacher_model, (3, 288, 384), name_of_model=\"Teacher (HRNet)\")\n",
    "evaluator.add_predictions_to_fiftyone(val_data, teacher_model, field_name=\"teacher_preds\")\n",
    "\n",
    "\n",
    "student_model_logits = SqueezeNetHPE(num_keypoints=17).to(device)\n",
    "student_model_logits.load_state_dict(torch.load('checkpoints_logits_kd/best_logit_kd_T4.0.pth', map_location=device))\n",
    "evaluator.count_trainable_parameters(student_model_logits, \"Student (Logit-KD)\")\n",
    "evaluator.add_predictions_to_fiftyone(val_data, student_model_logits, field_name=\"student_logit_kd\")\n",
    "\n",
    "\n",
    "student_model_baseline = SqueezeNetHPE(num_keypoints=17).to(device)\n",
    "student_model_baseline.load_state_dict(torch.load('checkpoints_baseline_kd/run_01/best_baseline_kl.pth', map_location=device))\n",
    "evaluator.count_trainable_parameters(student_model_baseline, \"Student baseline\")\n",
    "evaluator.add_predictions_to_fiftyone(val_data, student_model_baseline, field_name=\"student_baseline_kd\")\n",
    "\n",
    "# --- Launch Visualization ---\n",
    "session = fo.launch_app(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference_visualization_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a single batch\n",
    "images, targets, masks, _ = next(iter(val_loader))\n",
    "images = images.to(device)\n",
    "\n",
    "teacher_model.eval()\n",
    "student_model_logits.eval()\n",
    "student_model_baseline.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    t_out = teacher_model(images)\n",
    "    t_hms = t_out.heatmaps if hasattr(t_out, 'heatmaps') else t_out\n",
    "    t_preds, _ = evaluator._get_max_preds_vectorized(t_hms.cpu().numpy())\n",
    "\n",
    "    s_logit_out = student_model_logits(images)\n",
    "    s_logit_hms = s_logit_out.heatmaps \n",
    "    s_logit_preds, _ = evaluator._get_max_preds_vectorized(s_logit_hms.cpu().numpy())\n",
    "\n",
    "    s_base_out = student_model_baseline(images)\n",
    "    s_base_hms = s_base_out.heatmaps if hasattr(s_base_out, 'heatmaps') else s_base_out\n",
    "    s_base_preds, _ = evaluator._get_max_preds_vectorized(s_base_hms.cpu().numpy())\n",
    "\n",
    "\n",
    "print(\"--- Keypoint Predictions for First Image ---\")\n",
    "print(\"Teacher Keypoints:\\n\", t_preds[0])\n",
    "print(\"\\nStudent (Logit-KD) Keypoints:\\n\", s_logit_preds[0])\n",
    "print(\"\\nStudent (Baseline) Keypoints:\\n\", s_base_preds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdd18e3-8b94-4dd7-8959-dea428c670b3",
   "metadata": {},
   "source": [
    "## 3.3. Feature-based Knowledge Distillation\n",
    "\n",
    "Recently, it has been understood that rich information is normally located in the **intermediate layers** of a model. While logits capture the final outcome, intermediate features capture the structural and semantic hierarchies (e.g., edges, textures, and joint relationships) that the teacher has learned.\n",
    "\n",
    "Feature-based KD encourages the Student to learn intermediate representations that resemble the Teacher's. Since Student and Teacher features often have different dimensions in terms of both channels and spatial resolution, we typically use a **Connector** (also known as an **Adaptor**, e.g., $1 \\times 1$ Conv) to map Student features into the Teacher's feature space.\n",
    "\n",
    "**Feature Extraction**\n",
    "We utilize **Forward Hooks** to extract these intermediate feature maps during the forward pass without requiring structural modifications to the original model architectures.\n",
    "\n",
    "**Loss Function**\n",
    "The distillation loss for features is usually calculated using the Mean Squared Error (MSE) between the transformed student features and the teacher features. The total objective is defined as:\n",
    "\n",
    "$$L_{total} = L_{GT} + \\beta \\cdot L_{Feat}(F_{Adaptor}(F_S), F_T)$$\n",
    "\n",
    "Where:\n",
    "* $L_{GT}$ is the standard ground truth loss (e.g., MSE or KL Divergence).\n",
    "* $F_S$ and $F_T$ are the intermediate feature maps from the Student and Teacher, respectively.\n",
    "* $F_{Adaptor}$ is a learnable transformation (typically a $1 \\times 1$ Convolution) used to align the channel dimensions.\n",
    "* $\\beta$ is the weighting hyperparameter for the feature distillation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cka_imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.cka_implementation import CKACalculator\n",
    "from helper.cka_plots import plot_cka_heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cka_logit_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure models are in eval mode\n",
    "teacher_model.eval()\n",
    "student_model_logits.eval()\n",
    "\n",
    "print(\"Calculating CKA: Teacher vs Student (Logit KD)...\")\n",
    "cka_calc_logit = CKACalculator(\n",
    "    model1=teacher_model,\n",
    "    model2=student_model_logits,\n",
    "    dataloader=val_loader,\n",
    "    num_epochs=1,\n",
    "    is_main_process=True\n",
    ")\n",
    "cka_matrix_logit = cka_calc_logit.calculate_cka_matrix()\n",
    "\n",
    "plot_cka_heatmap(\n",
    "    cka_matrix=cka_matrix_logit,\n",
    "    model1_name='Teacher (HRNet)',\n",
    "    model2_name='Student (Logit KD)',\n",
    "    title=\"CKA: Teacher vs Student (Logit KD)\",\n",
    "    save_path='cka_teacher_vs_logit_kd.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cka_baseline_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model_baseline.eval()\n",
    "\n",
    "print(\"Calculating CKA: Teacher vs Student (Baseline)...\")\n",
    "cka_calc_base = CKACalculator(\n",
    "    model1=teacher_model,\n",
    "    model2=student_model_baseline,\n",
    "    dataloader=val_loader,\n",
    "    num_epochs=1,\n",
    "    is_main_process=True\n",
    ")\n",
    "cka_matrix_base = cka_calc_base.calculate_cka_matrix()\n",
    "\n",
    "plot_cka_heatmap(\n",
    "    cka_matrix=cka_matrix_base,\n",
    "    model1_name='Teacher (HRNet)',\n",
    "    model2_name='Student (Baseline)',\n",
    "    title=\"CKA: Teacher vs Student (Baseline)\",\n",
    "    save_path='cka_teacher_vs_baseline.png'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python(KD-venv)",
   "language": "python",
   "name": "kd_hpe_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
