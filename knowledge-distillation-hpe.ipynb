{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e3961cd-1d0b-42e8-b020-25d13b81c3c0",
   "metadata": {},
   "source": [
    "# Knowledge Distillation for 2D Human Pose Estimation\n",
    "\n",
    "## What is Knowledge Distillation?\n",
    "Knowledge Distillation (KD) is a technique in deep learning where a small, compact model (the **Student**) is trained to reproduce the behavior of a large, complex model (the **Teacher**), or an ensemble of models. The core idea was popularized by Hinton et al. in [\"Distilling the Knowledge in a Neural Network\" (2015)](https://arxiv.org/abs/1503.02531).\n",
    "\n",
    "In Human Pose Estimation (HPE), the Teacher is typically a high-performance model (like HRNet-W48) that is accurate but computationally expensive. The Student is a lightweight model (like SqueezeNet or MobileNet) designed for real-time inference on edge devices.\n",
    "\n",
    "## Why use KD?\n",
    "1. **Model Compression**: Reduce model size and inference latency.\n",
    "2. **Accuracy Boost**: The Student often learns better from the Teacher's \"soft targets\" (probability distributions or heatmaps) than from one-hot ground truth labels alone, as the Teacher provides structural information about the data.\n",
    "\n",
    "## Types of Knowledge Distillation covered here:\n",
    "1. **Response-based (Logits) KD**: The Student mimics the final output (heatmaps) of the Teacher.\n",
    "2. **Feature-based KD**: The Student mimics the intermediate feature maps of the Teacher, learning to extract similar spatial features.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52912d4b-0cbe-4d74-8451-ea941c7e96b1",
   "metadata": {
    "id": "52912d4b-0cbe-4d74-8451-ea941c7e96b1"
   },
   "source": [
    "# Introduction.\n",
    "What is KD, importance, why we need to learn from it, why is expect of the members to get.\n",
    "Requirements needed etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2aa767-4583-4fba-a59f-1a01988b981f",
   "metadata": {
    "id": "ac0f5f71-b364-4a64-90ee-510b04c08d00"
   },
   "source": [
    "#### Student model\n",
    "\n",
    "We design a lightweight model with <a href=\"https://arxiv.org/pdf/1602.07360\">SquuezeNet backbone</a> pretrained to extarct features from the an images. WSo we just remove tthe classification tail of the model and add a custom backbone for generating heatmaps.\n",
    "\n",
    "**SqueezeNet** is designed for efficiency. It achieves AlexNet-level accuracy with 50x fewer parameters. Its core building block is the **Fire Module**, which 'squeezes' the feature map depth with 1x1 filters before 'expanding' it with a mix of 1x1 and 3x3 filters.\n",
    "\n",
    "**Why use it as a Student?**\n",
    "- **Lightweight**: Ideal for deployment on mobile or embedded devices.\n",
    "- **Speed**: Fast inference times.\n",
    "- **Capacity gap**: It has significantly less capacity than HRNet, making it a perfect candidate to test the effectiveness of Knowledge Distillation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8bc2b59-20b3-4ead-b902-f59b6cd72cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple\n",
    "\n",
    "KeypointOutput = namedtuple('KeypointOutput', ['heatmaps'])\n",
    "class SqueezeNetHPE(nn.Module):\n",
    "    def __init__(self, num_keypoints=17):\n",
    "        super().__init__()\n",
    "        squeezenet = models.squeezenet1_1(weights=models.SqueezeNet1_1_Weights.IMAGENET1K_V1)\n",
    "        self.backbone = squeezenet.features\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv_heatmap = nn.Conv2d(\n",
    "            in_channels=512,\n",
    "            out_channels=num_keypoints,\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.relu(x)\n",
    "        x = F.interpolate(\n",
    "            x,\n",
    "            size=(96, 72), \n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False\n",
    "        )\n",
    "        heatmaps = self.conv_heatmap(x)\n",
    "\n",
    "        return KeypointOutput(heatmaps=heatmaps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43a8307-241a-450c-b3a5-25eabecc929d",
   "metadata": {},
   "source": [
    "The code above modifies the standard SqueezeNet classifier for the task of Pose Estimation (Heatmap Regression):\n",
    "\n",
    "1.  **Backbone (`self.backbone`)**: We use the feature extractor from `squeezenet1_1` pretrained on ImageNet. This provides rich, low-level visual features.\n",
    "2.  **Upsampling**: SqueezeNet significantly downsamples the input (typically by 32x). To generate high-quality heatmaps (which require spatial precision), we use `F.interpolate` to upsample the features to **96x72**.\n",
    "3.  **Heatmap Head (`self.conv_heatmap`)**: Instead of a Fully Connected layer for classification, we use a final **Convolutional Layer** (kernel size 3x3) to produce **17 output channels**, each corresponding to a keypoint heatmap.\n",
    "\n",
    "This simple \"Backbone + Upsample + 1x1 Conv\" structure is extremely lightweight/fast but often lacks the spatial refinement of HRNet, making it an ideal candidate for Knowledge Distillation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f96ebe4-39ed-4918-b5ac-c05eece46a61",
   "metadata": {},
   "source": [
    "#### Teacher model\n",
    "\n",
    "\n",
    "In this tutprial we will use <a href=\"https://github.com/HRNet/HRNet-Human-Pose-Estimation?tab=readme-ov-file\"> HRNet-pose as a teacher model </a>.\n",
    "\n",
    "**HRNet** (High-Resolution Net) maintains high-resolution representations throughout the network. Unlike traditional architectures (ResNet, VGG) that downsample the image and then upsample, HRNet connects high-to-low resolution subnetworks in parallel.\n",
    "\n",
    "**Key Features:**\n",
    "- **Parallel Resolutions**: Maintains high-res stream from start to finish.\n",
    "- **Multi-Scale Fusion**: Repeatedly exchanges information across resolutions.\n",
    "\n",
    "**Role as Teacher:**\n",
    "HRNet is a state-of-the-art CNN based model for 2D Pose Estimation. It captures fine spatial details (crucial for keypoints) which we want our Student to mimic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc40104a-fc60-426a-855c-46c1c7f7912f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82316/1909927884.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('hrnet_pose/hrnet_pose_models/pose_hrnet_w48_384x288.pth', map_location='cpu')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hrnet_pose.hrnetpose_model import get_pose_net\n",
    "from hrnet_pose.configs import load_configs\n",
    "\n",
    "cfg = load_configs('hrnet_pose/hrnet_w48_model_configs.yaml')\n",
    "teacher_model = get_pose_net(cfg, is_train=False)\n",
    "checkpoint = torch.load('hrnet_pose/hrnet_pose_models/pose_hrnet_w48_384x288.pth', map_location='cpu')\n",
    "teacher_model.load_state_dict(checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07f4e33-86e8-4556-83e8-5f0996fbb64f",
   "metadata": {
    "id": "e07f4e33-86e8-4556-83e8-5f0996fbb64f"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5e6a66-a521-4def-ae84-c9a39bed1b89",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### COCO Keypoint Dataset\n",
    "The COCO (Common Objects in Context) dataset is a large-scale object detection, segmentation, and captioning dataset. For Human Pose Estimation, we use the **Keypoints** task, which annotates **17 keypoints** (joints) on human bodies.\n",
    "\n",
    "The 17 Keypoints are:\n",
    "0: Nose, 1: Left Eye, 2: Right Eye, 3: Left Ear, 4: Right Ear, 5: Left Shoulder, 6: Right Shoulder, 7: Left Elbow, 8: Right Elbow, 9: Left Wrist, 10: Right Wrist, 11: Left Hip, 12: Right Hip, 13: Left Knee, 14: Right Knee, 15: Left Ankle, 16: Right Ankle.\n",
    "\n",
    "##### COCO Skeleton Visualization\n",
    "The keypoints are connected to form a skeleton structure, helping in visualizing pose.\n",
    "\n",
    "![COCO Skeleton](images/keypoints-skeleton.png) ![COCO Examples](images/keypoints-examples.png)\n",
    "\n",
    "*Example of COCO Keypoint Annotations (Source: COCO Dataset)*\n",
    "\n",
    "Access the <a href=\"https://cocodataset.org/#keypoints-2017\"> COCO 2017 keypoint dataset here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed95543-736a-435e-8e32-87e427398222",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 1. Accessing using fiftyone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc4fb73-1189-4304-8ed0-0c31ff32d748",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### What is FiftyOne?\n",
    "[FiftyOne](https://voxel51.com/fiftyone/) is an open-source toolset by Voxel51 designed for building high-quality datasets and computer vision models. It acts as a visual interface for your datasets, allowing you to:\n",
    "\n",
    "- **Visualize** raw images and their annotations (bounding boxes, keypoints, etc.) instantly.\n",
    "- **Query** specific samples (e.g., \"show me all images with > 10 people\").\n",
    "- **Debug** model predictions by overlaying them on ground truth.\n",
    "- **Manage** data splits and export to various formats (COCO, YOLO, TFRecord).\n",
    "\n",
    "In this notebook, we use FiftyOne to easily download specific subsets of COCO and visualize the ground truth keypoints interactively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35f3f3ef-3e88-4369-8cf7-98c591739ac2",
   "metadata": {
    "id": "35f3f3ef-3e88-4369-8cf7-98c591739ac2",
    "outputId": "f22f7827-0ae6-497b-b83f-76b1b58372a8"
   },
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b855961-7fae-4161-b88e-25dcddc26301",
   "metadata": {
    "id": "9b855961-7fae-4161-b88e-25dcddc26301",
    "outputId": "9e82d832-1bad-4b63-fd76-8cfc2f7c414f"
   },
   "outputs": [],
   "source": [
    "coco2017_train = foz.load_zoo_dataset(\"coco-2017\", split='train',max_samples=30000,  label_types=[\"person_keypoints\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc73542c-6b56-4aec-ac06-2faeddb3c058",
   "metadata": {
    "id": "dc73542c-6b56-4aec-ac06-2faeddb3c058",
    "outputId": "fbbf127a-6813-4dbd-9915-2fd02fba6b15"
   },
   "outputs": [],
   "source": [
    "coco2017_val = foz.load_zoo_dataset(\"coco-2017\", split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e44a357-7a11-4071-a95d-88821bcfa4a8",
   "metadata": {
    "id": "2e44a357-7a11-4071-a95d-88821bcfa4a8",
    "outputId": "4cce8813-f18e-4753-96a9-2d4db191daae"
   },
   "outputs": [],
   "source": [
    "coco2017_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd79bb20-0339-452d-a5b1-3cccd18ce6cf",
   "metadata": {
    "id": "bd79bb20-0339-452d-a5b1-3cccd18ce6cf",
    "outputId": "9b466a28-d275-448a-edf6-2573908bfe9a"
   },
   "outputs": [],
   "source": [
    "session = fo.launch_app(coco2017_train, auto=False)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9077f055-d120-43ee-9dab-28bc2d254225",
   "metadata": {
    "id": "045c85fd-69ba-4152-b87b-c54b96c559cb",
    "outputId": "8721e225-5afa-4c8a-e59f-76718515bbe7"
   },
   "source": [
    "session.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177333ad-6c81-4fcb-bf15-c7e5272240c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2. Direct download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ae12c2-ea69-4587-9389-3f57af6dd28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to store the data\n",
    "!mkdir -p /capstor/scratch/cscs/ckuya/coco_data /capstor/scratch/cscs/ckuya/coco_data/annotations \n",
    "\n",
    "!wget -c http://images.cocodataset.org/annotations/annotations_trainval2017.zip -P /capstor/scratch/cscs/ckuya/coco_data/\n",
    "!wget -c http://images.cocodataset.org/zips/val2017.zip -P /capstor/scratch/cscs/ckuya/coco_data/\n",
    "!wget -c http://images.cocodataset.org/zips/train2017.zip -P /capstor/scratch/cscs/ckuya/coco_data/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc524cf-358a-453b-bb1b-34f21c469a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip  /capstor/scratch/cscs/ckuya/coco_data/annotations_trainval2017.zip -d /capstor/scratch/cscs/ckuya/coco_data\n",
    "!unzip -q /capstor/scratch/cscs/ckuya/coco_data/val2017.zip -d /capstor/scratch/cscs/ckuya/coco_data/\n",
    "!unzip -q /capstor/scratch/cscs/ckuya/coco_data/train2017.zip -d /capstor/scratch/cscs/ckuya/coco_data/\n",
    "\n",
    "# Cleanup zip files\n",
    "!rm -rf /capstor/scratch/cscs/ckuya/coco_data/*.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeaef20-70c6-4bb2-a955-1cad726b23ca",
   "metadata": {
    "id": "dbeaef20-70c6-4bb2-a955-1cad726b23ca"
   },
   "source": [
    "### Data Prepocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9d733d-8a11-4f40-95ae-0d14343312be",
   "metadata": {
    "id": "ee9d733d-8a11-4f40-95ae-0d14343312be"
   },
   "source": [
    "1.  **Filtering**: We first check if an image actually contains a person with valid annotations. We filter out images where the number of annotated keypoints is zero.\n",
    "2.  **Cropping**: Once we identify a valid image, we locate the person using their bounding box and **crop** the image to center on them. This removes background noise and focuses the model's attention.\n",
    "3.  **Coordinate Transformation**: We transform the original keypoint annotations from the full image space into our new **cropped coordinate space**. This ensures the labels match the input image we feed to the network.\n",
    "4.  **Ground Truth Generation**: Finally, we take these transformed keypoints and generate **Gaussian Heatmaps**, which serve as the training targets for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7aa3f3b-708f-45ae-8c1f-3056e1937957",
   "metadata": {
    "id": "b7aa3f3b-708f-45ae-8c1f-3056e1937957"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple, Union\n",
    "import cv2\n",
    "\n",
    "def extract_keypoints_and_visibility(keypoints: List[float]):\n",
    "    try:\n",
    "        kps_array = np.array(keypoints, dtype=np.float32).reshape(-1, 3)\n",
    "        coords = kps_array[:,:2]\n",
    "        visibility = kps_array[:,2]\n",
    "\n",
    "        return coords, visibility.astype(np.int32)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting keypoints and visibility: {e}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "def process_image_and_keypoints(\n",
    "    image: np.ndarray,\n",
    "    keypoints: Union[List[List[float]], np.ndarray],\n",
    "    bbox: Union[List[int], Tuple[int, int, int, int]],\n",
    "    target_size: Tuple[int, int],\n",
    "    angle: float = 0,\n",
    "    flip: bool = False,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "    try:\n",
    "\n",
    "        x1, y1, w, h = np.round(bbox).astype(int)\n",
    "        img_h, img_w = image.shape[:2]\n",
    "\n",
    "        cx1, cy1 = max(0, x1), max(0, y1)\n",
    "        cx2, cy2 = min(img_w, x1 + w), min(img_h, y1 + h)\n",
    "\n",
    "        # Step 1: Crop\n",
    "        image = image[cy1:cy2, cx1:cx2]\n",
    "        crop_h, crop_w = image.shape[:2]\n",
    "\n",
    "        # Step 2: Prepare Keypoints (N, 2)\n",
    "        kps = np.ascontiguousarray(keypoints, dtype=np.float32).reshape(-1, 2)\n",
    "\n",
    "        zero_mask = np.all(kps == 0, axis=1)\n",
    "        kps -= [cx1, cy1]\n",
    "\n",
    "        # Step 3:Masking\n",
    "        invalid_mask = (kps[:, 0] < 0) | (kps[:, 1] < 0) | (kps[:, 0] >= crop_w) | (kps[:, 1] >= crop_h)\n",
    "\n",
    "        # Step 4: Resize (Image and KPs together)\n",
    "        image = cv2.resize(image, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "        kps *= [target_size[0] / crop_w, target_size[1] / crop_h]\n",
    "\n",
    "        # Step 5: Rotation \n",
    "        if angle != 0:\n",
    "            center = (target_size[0] / 2, target_size[1] / 2)\n",
    "            M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "            image = cv2.warpAffine(image, M, target_size)\n",
    "            kps = kps @ M[:, :2].T + M[:, 2]\n",
    "\n",
    "        # Step 6: Horizontal Flip\n",
    "        if flip:\n",
    "            image = cv2.flip(image, 1)\n",
    "            kps[:, 0] = target_size[0] - kps[:, 0]\n",
    "\n",
    "        # Step 7: Final Restoration\n",
    "        kps[zero_mask | invalid_mask] = 0\n",
    "\n",
    "        return image, kps\n",
    "    except Exception as e:\n",
    "        print(f\"Error in process_image_and_keypoints: {e}\")\n",
    "        raise e\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c9ea8a-67ee-4602-bffb-ae5c2a0e6e26",
   "metadata": {
    "id": "54c9ea8a-67ee-4602-bffb-ae5c2a0e6e26"
   },
   "source": [
    "#### Heatmaps\n",
    "\n",
    "Nowadays, **Gaussian Heatmaps** are the standard representation for 2D Pose Estimation. Instead of regressing exact (x, y) coordinates directly (which is hard to learn), the model predicts a probability map where the peak represents the keypoint location.\n",
    "\n",
    "**Generation Formula:**\n",
    "For a keypoint $k$ at $(x_k, y_k)$, the value at pixel $(i, j)$ is:\n",
    "\n",
    "$$H_k(i, j) = \\exp\\left( -\\frac{(i - x_k)^2 + (j - y_k)^2}{2\\sigma^2} \\right)$$\n",
    "\n",
    "Where $\\sigma$ controls the spread of the Gaussian peak.\n",
    "\n",
    "**Masks (Visibility):**\n",
    "We also associate a binary **Mask** with each keypoint. This mask is **1** if the keypoint is visible and annotated, and **0** otherwise. During training, we multiply the loss by this mask to essentially 'switch off' the learning for missing or invisible joints, preventing the model from being confused by unlabelled data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef5e05a1-8756-4449-aefa-7b9cc279ec37",
   "metadata": {
    "id": "ef5e05a1-8756-4449-aefa-7b9cc279ec37"
   },
   "outputs": [],
   "source": [
    "def generate_heatmaps(\n",
    "    keypoints: np.ndarray,\n",
    "    keypoints_visible: np.ndarray,\n",
    "    input_size: Tuple[int, int],\n",
    "    heatmap_size: Tuple[int, int] = (72, 96),\n",
    "    sigma: float = 2.0) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "    W, H = heatmap_size\n",
    "    w, h = input_size\n",
    "    K = keypoints.shape[0]\n",
    "\n",
    "    scale = np.array([(W - 1) / (w - 1), (H - 1) / (h - 1)], dtype=np.float32)\n",
    "    kps_hm = keypoints * scale  # Shape (K, 2)\n",
    "\n",
    "    mask = (keypoints_visible >= 0.5) & \\\n",
    "           (kps_hm[:, 0] >= 0) & (kps_hm[:, 0] < W) & \\\n",
    "           (kps_hm[:, 1] >= 0) & (kps_hm[:, 1] < H)\n",
    "    mask = mask.astype(np.float32)\n",
    "\n",
    "    yy, xx = np.meshgrid(np.arange(H), np.arange(W), indexing='ij')\n",
    "\n",
    "    mu_x = kps_hm[:, 0, np.newaxis, np.newaxis]\n",
    "    mu_y = kps_hm[:, 1, np.newaxis, np.newaxis]\n",
    "\n",
    "    dist_sq = (xx - mu_x) ** 2 + (yy - mu_y) ** 2\n",
    "\n",
    "    heatmaps = np.exp(-dist_sq / (2 * sigma**2))\n",
    "\n",
    "    heatmaps *= mask[:, np.newaxis, np.newaxis]\n",
    "\n",
    "    return torch.from_numpy(heatmaps), torch.from_numpy(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9ae3dd-2af1-487a-b2b7-47273275328d",
   "metadata": {
    "id": "ac9ae3dd-2af1-487a-b2b7-47273275328d"
   },
   "source": [
    "#### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea9cdf37-efbb-4436-b711-0ddeffd4b8cf",
   "metadata": {
    "id": "ea9cdf37-efbb-4436-b711-0ddeffd4b8cf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import torch\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pycocotools.coco import COCO\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Optional, Callable, List, Tuple\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_downloaded_image_ids(root_dir: str, coco: COCO) -> List[int]:\n",
    "    on_disk = set(os.listdir(root_dir))\n",
    "    all_img_info = coco.loadImgs(coco.getImgIds())\n",
    "    existing_ids = [info['id'] for info in all_img_info if info['file_name'] in on_disk]\n",
    "    return existing_ids\n",
    "\n",
    "class CocoKeypoints(Dataset):\n",
    "    def __init__(self, root: str, annFile: str, target_size=(288, 384), heatmap_size=(72, 96)) -> None:\n",
    "        self.root = root\n",
    "        self.coco = COCO(annFile)\n",
    "        self.target_size = target_size\n",
    "        self.heatmap_size = heatmap_size\n",
    "\n",
    "        img_ids = get_downloaded_image_ids(self.root, self.coco)\n",
    "        self.valid_anns = []\n",
    "\n",
    "        for img_id in img_ids:\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "            for ann in anns:\n",
    "                if ann.get(\"num_keypoints\", 0) > 0:\n",
    "                    self.valid_anns.append((img_id, ann))\n",
    "\n",
    "        logger.info(f\"Initialized {len(self.valid_anns)} person samples.\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.valid_anns)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "        img_id, target = self.valid_anns[index]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(self.root, img_info[\"file_name\"])\n",
    "        \n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        coords, visibility = extract_keypoints_and_visibility(target[\"keypoints\"])\n",
    "        coords_tensors = torch.from_numpy(coords)\n",
    "\n",
    "        processed_img_np, processed_kps = process_image_and_keypoints(\n",
    "            image, coords, target[\"bbox\"], self.target_size\n",
    "        )\n",
    "\n",
    "        hm_scale = [(self.heatmap_size[0]-1)/(self.target_size[0]-1),\n",
    "                    (self.heatmap_size[1]-1)/(self.target_size[1]-1)]\n",
    "        kps_hm = processed_kps * hm_scale\n",
    "\n",
    "        heatmaps, masks = generate_heatmaps(kps_hm, visibility, self.heatmap_size)\n",
    "        img_tensor = torch.from_numpy(processed_img_np).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        return img_tensor, heatmaps, masks, coords_tensors\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs, hms, masks, coords = zip(*batch)\n",
    "    return torch.stack(imgs), torch.stack(hms), torch.stack(masks), torch.stack(coords)\n",
    "\n",
    "def get_coco_dataloaders(root_train, ann_train, root_val, ann_val, batch_size=128, num_workers=8):\n",
    "    train_dataset = CocoKeypoints(root_train, ann_train)\n",
    "    val_dataset = CocoKeypoints(root_val, ann_val)\n",
    "    \n",
    "    loader_args = dict(\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        DataLoader(train_dataset, shuffle=True, **loader_args),\n",
    "        DataLoader(val_dataset, shuffle=False, **loader_args)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ab6c16b-98fc-4d76-8341-f026eeab3390",
   "metadata": {
    "id": "6ab6c16b-98fc-4d76-8341-f026eeab3390",
    "outputId": "dedb4965-6359-4ba8-adae-76b71f91bc18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=5.10s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 07:39:00,339 - INFO - Initialized 149813 person samples.\n",
      "2026-02-01 07:39:00,469 - INFO - Initialized 6352 person samples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.11s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "root_train = \"/capstor/scratch/cscs/ckuya/coco_data/train2017\"\n",
    "root_val = \"/capstor/scratch/cscs/ckuya/coco_data/val2017\"\n",
    "ann_train = \"/capstor/scratch/cscs/ckuya/coco_data/annotations/person_keypoints_train2017.json\"\n",
    "ann_val = \"/capstor/scratch/cscs/ckuya/coco_data/annotations/person_keypoints_val2017.json\"\n",
    "\n",
    "train_loader, val_loader = get_coco_dataloaders(\n",
    "    root_train=root_train,\n",
    "    ann_train=ann_train,\n",
    "    root_val=root_val,\n",
    "    ann_val=ann_val,\n",
    "    batch_size=128,\n",
    "    num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68640d8d-06b4-4bad-ad33-c8f771aa3d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(\n",
    "    student_model: nn.Module,\n",
    "    val_loader: torch.utils.data.DataLoader,\n",
    "    criterion: nn.Module,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Validate the model against ground truth heatmaps only.\n",
    "    \"\"\"\n",
    "    student_model.eval()\n",
    "    total_val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, gt_heatmaps, _, _ in tqdm(val_loader, desc=\"Validation process\"):\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            heatmaps = gt_heatmaps.to(device, non_blocking=True)\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type):\n",
    "                outputs = student_model(imgs)\n",
    "                student_heatmaps = outputs.heatmaps\n",
    "\n",
    "                loss = criterion(student_heatmaps, heatmaps)\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    return total_val_loss / len(val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644d54e8-db26-4ea7-971c-f820c0110aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters_and_flops(student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e428113-7c66-4089-acec-4ae710550ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heatmaps_to_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566bf1b6-307d-4830-88cf-8ae5cfad8a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heatmaps_to_keypoints\n",
    "results = heatmaps_to_keypoint.decode(gt_ht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dc5d7c-283a-4d81-a90d-a727814ba94a",
   "metadata": {
    "id": "a1dc5d7c-283a-4d81-a90d-a727814ba94a"
   },
   "outputs": [],
   "source": [
    "out_s = student_model(imgb.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c678a82-e0f3-4089-a4d1-9ec7482321ea",
   "metadata": {
    "id": "2c678a82-e0f3-4089-a4d1-9ec7482321ea",
    "outputId": "e537f760-2cd3-4691-a1c8-402e50dfe3d2"
   },
   "outputs": [],
   "source": [
    "out_s.heatmaps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeb6790-d14b-4ee6-93fd-a3fd907b2da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model.to(device).eval()\n",
    "out_t = teacher_model(imgb.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62702d8c-33db-4d88-89af-806cde700003",
   "metadata": {},
   "source": [
    "### Training and distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586def00-6565-4c0c-8d7f-5e84a62a0b99",
   "metadata": {},
   "source": [
    "#### 1. Baseline Student Training\n",
    "\n",
    "We start by training the **Student Model (SqueezeNet)** purely on the Ground Truth labels, **without any distillation**.\n",
    "\n",
    "This establishes a **baseline performance**. It represents how well the lightweight model can learn the task on its own. Later, we will compare our distilled models against this baseline to measure the improvement gained from the Teacher's guidance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b606189-a609-4d68-858f-b9a81cbd3c10",
   "metadata": {
    "id": "5b606189-a609-4d68-858f-b9a81cbd3c10"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "student_model = SqueezeNetHPE(num_keypoints=17).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51b26d79-877d-4bf2-8519-19ed98cffcbe",
   "metadata": {
    "id": "51b26d79-877d-4bf2-8519-19ed98cffcbe"
   },
   "outputs": [],
   "source": [
    "class AdaptiveWingLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        omega: float = 14.0,\n",
    "        theta: float = 0.5,\n",
    "        epsilon: float = 1.0,\n",
    "        alpha: float = 2.1,\n",
    "        weight: float = 10.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.omega = omega\n",
    "        self.theta = theta\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.weight = weight\n",
    "        self.inv_epsilon = 1.0 / epsilon\n",
    "\n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        delta = (pred - target).abs()\n",
    "        y_pow = self.alpha - target\n",
    "\n",
    "        theta_eps_pow = torch.pow(self.theta * self.inv_epsilon, y_pow)\n",
    "\n",
    "        A = (self.omega * (1.0 / (1.0 + theta_eps_pow))\n",
    "             * y_pow * torch.pow(self.theta * self.inv_epsilon, y_pow - 1.0)\n",
    "             * self.inv_epsilon)\n",
    "\n",
    "        C = self.theta * A - self.omega * torch.log1p(theta_eps_pow)\n",
    "\n",
    "        losses = torch.where(\n",
    "            delta < self.theta,\n",
    "            self.omega * torch.log1p(torch.pow(delta * self.inv_epsilon, y_pow)),\n",
    "            A * delta - C\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            Hd = F.max_pool2d(target, kernel_size=3, stride=1, padding=1)\n",
    "            importance_mask = (Hd >= 0.2).to(pred.dtype)\n",
    "\n",
    "        weighted_losses = losses * (self.weight * importance_mask + 1.0)\n",
    "\n",
    "        return weighted_losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31f0f05d-dd05-4e92-81ee-0357e81e3323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/capstor/scratch/cscs/ckuya/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_66858/3056113005.py:20: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Baseline Training on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/1171 [00:00<?, ?it/s]/tmp/ipykernel_66858/3056113005.py:39: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1/3: 100%|██████████| 1171/1171 [01:37<00:00, 12.02it/s, loss=0.221, lr=0.001]\n",
      "Epoch 2/3: 100%|██████████| 1171/1171 [01:31<00:00, 12.75it/s, loss=0.162, lr=0.001]\n",
      "Epoch 3/3: 100%|██████████| 1171/1171 [01:31<00:00, 12.78it/s, loss=0.162, lr=0.0005]\n",
      "Validation process: 100%|██████████| 50/50 [00:05<00:00,  8.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy (Epoch 3): 16.21%\n",
      "Saved checkpoint for epoch 3\n",
      "New Best Model Saved! (Acc: 16.21%)\n",
      "Baseline Training Completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "\n",
    "\n",
    "checkpoint_dir = 'checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "criterion = AdaptiveWingLoss().to(device)\n",
    "optimizer = optim.AdamW(student_model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "num_epochs = 3\n",
    "warmup_epochs = 1\n",
    "best_acc = 0.0\n",
    "\n",
    "lambda_lr = lambda epoch: (epoch + 1) / warmup_epochs if epoch < warmup_epochs else 0.5 * (1 + math.cos(math.pi * (epoch - warmup_epochs) / (num_epochs - warmup_epochs)))\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_lr)\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "print(f\"Starting Baseline Training on {device}...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    student_model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=True)\n",
    "    \n",
    "    for images, targets, masks, _ in pbar:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        masks = masks.to(device, non_blocking=True)\n",
    "        masks_expanded = masks.unsqueeze(-1).unsqueeze(-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed Precision Context\n",
    "        with autocast():\n",
    "            outputs = student_model(images)\n",
    "            preds = outputs.heatmaps\n",
    "            \n",
    "            loss = criterion(preds * masks_expanded, targets * masks_expanded)\n",
    "        \n",
    "        # Backward pass with scaler\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': running_loss / (pbar.n + 1), 'lr': optimizer.param_groups[0]['lr']})\n",
    "    \n",
    "    # Step Scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Evaluation & Checkpointing\n",
    "    if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:\n",
    "        val_acc = validate(student_model, val_loader, criterion)\n",
    "        print(f\"Validation Accuracy (Epoch {epoch+1}): {val_acc*100:.2f}%\")\n",
    "        \n",
    "        # Save Periodic Checkpoint\n",
    "        torch.save(student_model.state_dict(), os.path.join(checkpoint_dir, f'student_epoch_{epoch+1}.pth'))\n",
    "        print(f\"Saved checkpoint for epoch {epoch+1}\")\n",
    "        \n",
    "        # Save Best Model\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(student_model.state_dict(), os.path.join(checkpoint_dir, 'student_best.pth'))\n",
    "            print(f\"New Best Model Saved! (Acc: {best_acc*100:.2f}%)\")\n",
    "    \n",
    "print(\"Baseline Training Completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5194ca6c-fcf5-4f83-9b83-7ef3943d9090",
   "metadata": {},
   "source": [
    "#### Evaluate Baseline Student\n",
    "We evaluate the trained student model in two ways:\n",
    "1.  **Qualitative Accuracy (Visual)**: Using FiftyOne to visualize model predictions vs Ground Truth.\n",
    "2.  **Model Complexity**: Using `calflops` to measure FLOPs and Parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aceb5b8-c9b5-40b1-b872-5e59488f0968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe0bcfd-2067-447e-9c99-5814f5c419e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_predictions_to_fiftyone(dataset, model, num_samples=10):\n",
    "    model.eval()\n",
    "    print(\"Generating predictions for FiftyOne...\")\n",
    "    view = dataset.take(num_samples)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sample in view.iter_samples(autosave=True):\n",
    "            # Load and Preprocess image (simplified for demo)\n",
    "            # Note: In a real pipeline, we'd use the exact Transform pipeline.\n",
    "            # Here we just assume we can load it.\n",
    "            img = cv2.imread(sample.filepath)\n",
    "            if img is None:\n",
    "                continue\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)a            h, w, _ = img.shape\n",
    "            \n",
    "            # Resize to Model Input (288x384)\n",
    "            img_resized = cv2.resize(img, (288, 384))\n",
    "            input_tensor = torch.from_numpy(img_resized).permute(2,0,1).float() / 255.0\n",
    "            input_tensor = input_tensor.unsqueeze(0).to(device)\n",
    "            \n",
    "            # Inference\n",
    "            output = model(input_tensor)\n",
    "            heatmaps = output.heatmaps\n",
    "            \n",
    "            # Decode\n",
    "            preds, _ = get_max_preds(heatmaps.cpu())\n",
    "            preds = preds[0] # First batch item\n",
    "            \n",
    "            # Scale back to Original Image Dimensions\n",
    "            # Heatmap size is 72x96\n",
    "            # scale_x = w / 72.0\n",
    "            # scale_y = h / 96.0\n",
    "            \n",
    "            points = []\n",
    "            for node_i in range(17):\n",
    "                 x, y, conf = preds[node_i]\n",
    "                 # Predictions are in Heatmap coords\n",
    "                 # FiftyOne expects Normalized coords (0-1)\n",
    "                 norm_x = (x / 72.0)\n",
    "                 norm_y = (y / 96.0)\n",
    "                 \n",
    "                 # Clip to 0-1 to avoid plotting errors\n",
    "                 norm_x = max(0, min(1, norm_x))\n",
    "                 norm_y = max(0, min(1, norm_y))\n",
    "                 \n",
    "                 points.append((norm_x, norm_y))\n",
    "            \n",
    "            # Add to Sample\n",
    "            sample['predictions'] = fo.Keypoints(keypoints=[fo.Keypoint(points=points)])\n",
    "    \n",
    "    return view\n",
    "\n",
    "# Visualizing Predictions\n",
    "val_view = add_predictions_to_fiftyone(coco2017_val, student_model, num_samples=20)\n",
    "session = fo.launch_app(val_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef0a545-e4ca-41a2-b986-e8fedd31407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from calflops import calculate_flops\n",
    "\n",
    "input_shape = (1, 3, 384, 288)\n",
    "flops, macs, params = calculate_flops(model=student_model, input_shape=input_shape)\n",
    "\n",
    "print(f\"Student Model Complexity:\")\n",
    "print(f\" - FLOPs: {flops}\")\n",
    "print(f\" - MACs:  {macs}\")\n",
    "print(f\" - Params: {params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf398ddc-f226-4f0d-ba48-7ff0eff02476",
   "metadata": {},
   "source": [
    "# 2. Logits-based Knowledge Distillation\n",
    "\n",
    "## Theory: Hinton's KD with Temperature\n",
    "In the seminal work by Hinton et al., Knowledge Distillation softens the logits of the Teacher to provide more information about the class distribution.\n",
    "\n",
    "The distillation loss is defined as the Kullback-Leibler (KL) Divergence between the softened student logits $z_s$ and teacher logits $z_t$, scaled by temperature $T^2$:\n",
    "\n",
    "$$ L_{KD} = T^2 \\cdot KL\\left( \\sigma\\left(\\frac{z_s}{T}\\right), \\sigma\\left(\\frac{z_t}{T}\\right) \\right) $$\n",
    "\n",
    "Where $\\sigma$ is the softmax function.\n",
    "\n",
    "**In Human Pose Estimation (Regression/Heatmaps):**\n",
    "Typically, MSE is used for heatmaps. However, we can treat the heatmaps as **Spatial Probability Distributions** by flattening the spatial dimensions ($H \\times W$) and applying Softmax. This allows us to use Hinton's formulation directly by softening the peaky heatmap distributions.\n",
    "\n",
    "**Loss Function:**\n",
    "$$ L_{total} = L_{GT} + \\alpha \\cdot T^2 \\cdot KL(\\text{SpatialSoftmax}(H_S/T), \\text{SpatialSoftmax}(H_T/T)) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdf6d52-ca1a-40e5-8c4e-c0ab9b52f949",
   "metadata": {},
   "source": [
    "### Understanding Temperature ($T$) in Logits Loss\n",
    "\n",
    "You might wonder: **Why do we divide logits by a Temperature $T$?**\n",
    "\n",
    "The standard Softmax function is defined as:\n",
    "$$ \\sigma(z_i) = \\frac{\\exp(z_i)}{\\sum_j \\exp(z_j)} $$\n",
    "\n",
    "When a model is very confident, one value in inputs $z$ is much larger than the others. This makes the output probability distribution **extremely sharp** (almost 1.0 for the peak, and 0.0 for everything else).\n",
    "\n",
    "**The Problem:** If the Teacher predicts a perfect 1.0 for the correct keypoint and 0.0 everywhere else, it provides **no more information** than the Ground Truth labels! We lose the rich structural knowledge (e.g., \"this pixel is 0.001 likely, but that pixel is 0.05 likely\").\n",
    "\n",
    "**The Solution (Temperature):**\n",
    "By introducing $T > 1$, we \"soften\" the distribution:\n",
    "$$ q_i = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)} $$\n",
    "\n",
    "- **High T (> 1)**: Flattens the peaks. The Teacher's output becomes softer, spreading probability mass to neighboring pixels. This reveals the **\"Dark Knowledge\"**—the relationships between the peak and its surrounding area.\n",
    "- **Low T (1)**: Standard sharp Softmax.\n",
    "\n",
    "**In our Code:**\n",
    "We implement this in `spatial_kl_loss`. We take the raw heatmap logits (before softmax), divide them by `temp`, and *then* apply Softmax. This forces the Student to learn not just the peak location, but the entire spatial uncertainty shape of the Teacher.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6beadef-0f70-4878-8ef1-a6e80634e1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os, math\n",
    "from tqdm import tqdm\n",
    "\n",
    "teacher_model = teacher_model.to(device).eval()\n",
    "student_model_logits = SqueezeNetHPE(num_keypoints=17).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(student_model_logits.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "lambda_lr = lambda epoch: (epoch + 1) / 5 if epoch < 5 else 0.5 * (1 + math.cos(math.pi * (epoch - 5) / 25))\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_lr)\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "criterion = AdaptiveWingLoss().to(device)\n",
    "def spatial_kl_loss(s_logits, t_logits, temp=4.0):\n",
    "    B, K, H, W = s_logits.shape\n",
    "    s_prob = F.log_softmax(s_logits.view(B, K, -1) / temp, dim=-1)\n",
    "    t_prob = F.softmax(t_logits.view(B, K, -1) / temp, dim=-1)\n",
    "    return F.kl_div(s_prob, t_prob, reduction='batchmean') * (temp**2)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "checkpoint_dir_kd = 'checkpoints_logits_kd'\n",
    "os.makedirs(checkpoint_dir_kd, exist_ok=True)\n",
    "\n",
    "for epoch in range(30):\n",
    "    student_model_logits.train()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f'KD Epoch {epoch+1}/30')\n",
    "    \n",
    "    for images, targets, masks, _ in pbar:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        masks_expanded = masks.to(device, non_blocking=True).unsqueeze(-1).unsqueeze(-1)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        with torch.amp.autocast('cuda'):\n",
    "            with torch.no_grad():\n",
    "                t_heatmaps = teacher_model(images)\n",
    "            \n",
    "            s_out = student_model_logits(images)\n",
    "            s_heatmaps = s_out.heatmaps\n",
    "            \n",
    "            loss_gt = criterion(s_heatmaps * masks_expanded, targets * masks_expanded)\n",
    "            loss_kd = spatial_kl_loss(s_heatmaps, t_heatmaps, temp=4.0)\n",
    "            loss = loss_gt + (1.0 * loss_kd)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # Updated validation call to match your specific function signature\n",
    "    val_loss = validate(student_model_logits, val_loader, criterion)\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {running_loss/len(train_loader):.6f} | Val Loss: {val_loss:.6f}\")\n",
    "    \n",
    "    # Save if validation loss improves (decreases)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(student_model_logits.state_dict(), os.path.join(checkpoint_dir_kd, 'best_model.pth'))\n",
    "        print(f\"-> Best model saved (Val Loss: {val_loss:.6f})\")\n",
    "\n",
    "print(f\"Training Complete. Best Val Loss: {best_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c9192f-a726-4bb2-9b8f-3b3ac3e12ac4",
   "metadata": {},
   "source": [
    "### Evaluate Logits KD Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c97213-a3d0-4510-9ac6-22e2c1dd87a0",
   "metadata": {},
   "source": [
    "We now evaluate the performance of the Student model trained with Logits-based Knowledge Distillation.\n",
    "Compare this accuracy with the Baseline Student to see the improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6acfd0a-766e-484e-80af-d5cac835540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating Logits-KD Student...\")\n",
    "acc_kd = validate(None, val_loader, coco2017_val, student_model_logits)\n",
    "print(f'Logits KD Student Accuracy (Approx PCK): {acc_kd*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd3efee-cc19-40f5-9fc3-0c03040da64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch a new session or update existing\n",
    "# Note: You might need to close the previous session manually in the UI if it conflicts,\n",
    "# but fiftyone usually handles multiple sessions fine or we can reuse `session`.\n",
    "\n",
    "print(\"Visualizing Logits-KD predictions...\")\n",
    "val_view_kd = add_predictions_to_fiftyone(coco2017_val, student_model_logits, num_samples=20)\n",
    "session_kd = fo.launch_app(val_view_kd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdd18e3-8b94-4dd7-8959-dea428c670b3",
   "metadata": {},
   "source": [
    "# 3. Feature-based Knowledge Distillation\n",
    "\n",
    "## Theory\n",
    "Feature-based KD encourages the Student to learn intermediate representations that resemble the Teacher's. Since Student and Teacher features often have different dimensions (channels/resolution), we typically use a **Connector** (e.g., 1x1 Conv) to map Student features to the Teacher's space.\n",
    "\n",
    "We will use **Forward Hooks** to extract intermediate feature maps.\n",
    "\n",
    "**Loss Function:**\n",
    "$$ L_{total} = L_{GT} + \\beta L_{Feat}(F_{Adaptor}(F_S), F_T) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "068e4124-49da-4d86-83de-fd1eb9d76fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hooks defined (Need to register on specific layers depending on model struct)\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction using Hooks\n",
    "\n",
    "teacher_features = {}\n",
    "student_features = {}\n",
    "\n",
    "def get_activation(name, storage_dict):\n",
    "    def hook(model, input, output):\n",
    "        storage_dict[name] = output\n",
    "    return hook\n",
    "\n",
    "# Register Hooks\n",
    "# We need to know the layer names. For SqueezeNet/HRNet, we pick a mid-level layer.\n",
    "# Example: 'features.12' for SqueezeNet (Fire8) and a corresponding stage for HRNet.\n",
    "\n",
    "# Note: You must check your specific model structure (print(model)) to choose layers.\n",
    "# Here, we assume generic names for demonstration.\n",
    "\n",
    "# teacher_model.layerX.register_forward_hook(get_activation('mid_layer', teacher_features))\n",
    "# student_model.features[12].register_forward_hook(get_activation('mid_layer', student_features))\n",
    "\n",
    "print(\"Hooks defined (Need to register on specific layers depending on model struct)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "806e802e-ff9b-4772-a74c-f840a59e0e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hooks to capture features\n",
    "teacher_features = {}\n",
    "student_features = {}\n",
    "\n",
    "def get_activation(name, storage):\n",
    "    def hook(model, input, output):\n",
    "        storage[name] = output\n",
    "    return hook\n",
    "\n",
    "# Register hooks\n",
    "# Inspect models to find good layers. For SqueezeNet, maybe 'features.12' (Fire module).\n",
    "# For HRNet, standard layers.\n",
    "# Here we assume we looked at the architecture. Let's pick a mid-level feature.\n",
    "\n",
    "# Checking SqueezeNet structure\n",
    "# print(student_model)\n",
    "# SqueezeNet: features is a Sequential. 'features.10' is a Fire module.\n",
    "\n",
    "handle_t = teacher_model.layer1.register_forward_hook(get_activation('feat', teacher_features)) if hasattr(teacher_model, 'layer1') else None\n",
    "handle_s = student_model.backbone[10].register_forward_hook(get_activation('feat', student_features))\n",
    "\n",
    "if handle_t is None:\n",
    "    # Fallback if layer name differs (common in different HRNet impls)\n",
    "    # Using a dummy forward to find a valid layer if needed, but for now we trust `layer1` or similar exists\n",
    "    # If HRNet structure is complex, we might skip or assume a specific index.\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a73c35-0c0c-4a07-a22e-429be1a76c46",
   "metadata": {},
   "source": [
    "Note: We need a **Feature Adaptor** to match dimensions. Let's assume we capture features and define an adaptor dynamically or pre-set it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c710df35-09c1-41fc-ab7b-ec67426bc7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Feature-based KD Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature KD Epoch 1/10:  12%|█▏        | 145/1171 [00:29<03:25,  4.99it/s, loss=0.1763] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     64\u001b[39m     total_loss = loss_gt + (\u001b[32m0.5\u001b[39m * loss_feat)\n\u001b[32m     66\u001b[39m scaler.scale(total_loss).backward()\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m scaler.update()\n\u001b[32m     70\u001b[39m running_loss += total_loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/capstor/scratch/cscs/ckuya/.venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:457\u001b[39m, in \u001b[36mGradScaler.step\u001b[39m\u001b[34m(self, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    451\u001b[39m     \u001b[38;5;28mself\u001b[39m.unscale_(optimizer)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    454\u001b[39m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m]) > \u001b[32m0\u001b[39m\n\u001b[32m    455\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m457\u001b[39m retval = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m] = OptState.STEPPED\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/capstor/scratch/cscs/ckuya/.venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:351\u001b[39m, in \u001b[36mGradScaler._maybe_opt_step\u001b[39m\u001b[34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    344\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    345\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    348\u001b[39m     **kwargs: Any,\n\u001b[32m    349\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    350\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v.item() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m    352\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/capstor/scratch/cscs/ckuya/.venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:351\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    344\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    345\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    348\u001b[39m     **kwargs: Any,\n\u001b[32m    349\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    350\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m    352\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os, math\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Ensure models are on the correct device and teacher is in eval mode\n",
    "teacher_model = teacher_model.to(device).eval()\n",
    "student_model_feat = SqueezeNetHPE(num_keypoints=17).to(device)\n",
    "\n",
    "# FeatureAdapter to match student channels to teacher channels\n",
    "# Adjust in_channels/out_channels based on your specific model layers\n",
    "adapter = nn.Conv2d(in_channels=512, out_channels=48, kernel_size=1).to(device)\n",
    "\n",
    "# Optimization: Combine parameters for the optimizer\n",
    "optimizer = optim.AdamW(list(student_model_feat.parameters()) + list(adapter.parameters()), lr=1e-3)\n",
    "lambda_lr = lambda epoch: (epoch + 1) / 5 if epoch < 5 else 0.5 * (1 + math.cos(math.pi * (epoch - 5) / 5))\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_lr)\n",
    "criterion = AdaptiveWingLoss().to(device)\n",
    "# Modern AMP setup\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "checkpoint_dir_feat = 'checkpoints_feat_kd'\n",
    "os.makedirs(checkpoint_dir_feat, exist_ok=True)\n",
    "\n",
    "print(\"--- Starting Feature-based KD Training ---\")\n",
    "\n",
    "for epoch in range(10):\n",
    "    student_model_feat.train()\n",
    "    adapter.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f'Feature KD Epoch {epoch+1}/10')\n",
    "    \n",
    "    for imgs, targets, masks, _ in pbar:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        masks_expanded = masks.to(device, non_blocking=True).unsqueeze(-1).unsqueeze(-1)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        with torch.amp.autocast('cuda'):\n",
    "            # Teacher forward (No gradients)\n",
    "            with torch.no_grad():\n",
    "                t_out = teacher_model(imgs)\n",
    "                t_activation = t_out # Adjust this if using hooks for intermediate layers\n",
    "            \n",
    "            # Student forward\n",
    "            outputs = student_model_feat(imgs)\n",
    "            s_heatmaps = outputs.heatmaps\n",
    "            s_activation = s_heatmaps # Placeholder; adjust if using intermediate features\n",
    "            \n",
    "            # Adapt student features to teacher spatial/channel dimensions\n",
    "            s_adapted = adapter(s_activation) if s_activation.shape != t_activation.shape else s_activation\n",
    "            \n",
    "            # 1. Ground Truth Loss\n",
    "            loss_gt = criterion(s_heatmaps * masks_expanded, targets * masks_expanded)\n",
    "            \n",
    "            # 2. Feature-based MSE Loss\n",
    "            loss_feat = mse_loss(s_adapted, t_activation)\n",
    "            \n",
    "            total_loss = loss_gt + (0.5 * loss_feat)\n",
    "        \n",
    "        scaler.scale(total_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_loss += total_loss.item()\n",
    "        pbar.set_postfix({'loss': f\"{total_loss.item():.4f}\"})\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation using your heatmap-level loss function\n",
    "    val_loss = validate(student_model_feat, val_loader, criterion)\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {running_loss/len(train_loader):.6f} | Val Loss: {val_loss:.6f}\")\n",
    "    \n",
    "    # Checkpoint based on validation loss improvement\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(student_model_feat.state_dict(), os.path.join(checkpoint_dir_feat, 'best_feat_model.pth'))\n",
    "        print(f\"-> Best model saved (Val Loss: {val_loss:.6f})\")\n",
    "\n",
    "print(f\"Feature KD Training Completed. Best Val Loss: {best_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f03e76-c191-4b01-87e7-044dc5e8ec38",
   "metadata": {},
   "source": [
    "### Evaluate Feature KD Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dca6f11-aaa7-4905-b7c2-2ac28787bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Final Results ---\")\n",
    "print(f\"Baseline Student:     {best_acc*100:.2f}%\")\n",
    "print(f\"Logits-KD Student:    {best_acc_kd*100:.2f}%\")\n",
    "print(f\"Feature-KD Student:   {best_acc_feat*100:.2f}%\")\n",
    "\n",
    "# Teacher Eval (Reference)\n",
    "acc_teacher = validate(None, val_loader, coco2017_val, teacher_model)\n",
    "print(f\"Teacher (HRNet):      {acc_teacher*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3071fc45-e417-47b5-8f3e-2e4a4c9a28a6",
   "metadata": {},
   "source": [
    "# Conclusion Results Comparison\n",
    "Let's compare the performance of all three approaches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554ba013-c39d-493c-a4ea-932ebd03dfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Baseline: {acc*100:.2f}%')\n",
    "print(f'Logits KD: {acc_logits*100:.2f}%')\n",
    "print(f'Feature KD: {acc_feat*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417a5cb0-78f0-4c3c-a840-1af738f8e6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python(DSAIL-VENV)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
