{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52912d4b-0cbe-4d74-8451-ea941c7e96b1",
   "metadata": {},
   "source": [
    "# Introduction.\n",
    "What is KD, importance, why we need to learn from it, why is expect of the members to get.\n",
    "Requirements needed etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07f4e33-86e8-4556-83e8-5f0996fbb64f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97ccf1d-c535-444f-ad48-5c9457b6bcc6",
   "metadata": {},
   "source": [
    "In this tutorial, we will use <a href=\"https://cocodataset.org/#keypoints-2017\"> COCO 2017 keypoint dataset</a> and use <a href=\"https://docs.voxel51.com/#where-to-begin\"> fiftyone </a> for access and visualization of the the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "35f3f3ef-3e88-4369-8cf7-98c591739ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8cd61ffa-8434-4e6a-aab8-146f3028b779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'fiftyone' from '/home/clinton-mwangi/Desktop/projects/2d-human-pose-estimation-using-kd/kd_hpe_env/lib/python3.12/site-packages/fiftyone/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "print(fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efd51a4-c30d-4698-bce3-2ec39502b29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "foz.list_zoo_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9b855961-7fae-4161-b88e-25dcddc26301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'train' to '/home/clinton-mwangi/fiftyone/coco-2017/train' if necessary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 11:41:16,871 - INFO - Downloading split 'train' to '/home/clinton-mwangi/fiftyone/coco-2017/train' if necessary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found annotations at '/home/clinton-mwangi/fiftyone/coco-2017/raw/instances_train2017.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 11:41:16,876 - INFO - Found annotations at '/home/clinton-mwangi/fiftyone/coco-2017/raw/instances_train2017.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sufficient images already downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 11:41:39,660 - INFO - Sufficient images already downloaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing download of split 'train' is sufficient\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 11:41:42,465 - INFO - Existing download of split 'train' is sufficient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing dataset 'coco-2017-train-30000'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 11:41:42,470 - INFO - Loading existing dataset 'coco-2017-train-30000'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    }
   ],
   "source": [
    "coco2017_train = foz.load_zoo_dataset(\"coco-2017\", split='train',max_samples=30000,  label_types=[\"person_keypoints\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc73542c-6b56-4aec-ac06-2faeddb3c058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'validation' to '/home/clinton-mwangi/fiftyone/coco-2017/validation' if necessary\n",
      "Found annotations at '/home/clinton-mwangi/fiftyone/coco-2017/raw/instances_val2017.json'\n",
      "Found 1000 (< 5000) downloaded images; must download full image zip\n",
      "Downloading images to '/home/clinton-mwangi/fiftyone/coco-2017/tmp-download/val2017.zip'\n",
      " 100% |██████|    6.1Gb/6.1Gb [49.1m elapsed, 0s remaining, 2.3Mb/s]       \n",
      "Extracting images to '/home/clinton-mwangi/fiftyone/coco-2017/validation/data'\n",
      "Writing annotations to '/home/clinton-mwangi/fiftyone/coco-2017/validation/labels.json'\n",
      "Dataset info written to '/home/clinton-mwangi/fiftyone/coco-2017/info.json'\n",
      "Loading 'coco-2017' split 'validation'\n",
      " 100% |███████████████| 5000/5000 [23.5s elapsed, 0s remaining, 98.4 samples/s]       \n",
      "Dataset 'coco-2017-validation' created\n"
     ]
    }
   ],
   "source": [
    "coco2017_val = foz.load_zoo_dataset(\"coco-2017\", split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e44a357-7a11-4071-a95d-88821bcfa4a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name:        coco-2017-validation-1000\n",
       "Media type:  image\n",
       "Num samples: 1000\n",
       "Persistent:  False\n",
       "Tags:        []\n",
       "Sample fields:\n",
       "    id:               fiftyone.core.fields.ObjectIdField\n",
       "    filepath:         fiftyone.core.fields.StringField\n",
       "    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
       "    created_at:       fiftyone.core.fields.DateTimeField\n",
       "    last_modified_at: fiftyone.core.fields.DateTimeField\n",
       "    ground_truth:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco2017_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd79bb20-0339-452d-a5b1-3cccd18ce6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(coco2017_train, auto=False) #False here make use to not have the app in the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f398c4-7281-4688-8f09-b31594487385",
   "metadata": {},
   "outputs": [],
   "source": [
    "session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0e583e-8754-4322-b047-1efc327b61c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9d733d-8a11-4f40-95ae-0d14343312be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0014313-1434-46d1-a8b3-c0b60900cfcd",
   "metadata": {},
   "source": [
    "## Distillation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7bae9b-c107-4efb-b452-c7d8195cd9de",
   "metadata": {},
   "source": [
    "### Teacher model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf148139-d1b4-480d-a335-9f6bb948cda2",
   "metadata": {},
   "source": [
    "In this tutprial we will use <a href=\"https://github.com/HRNet/HRNet-Human-Pose-Estimation?tab=readme-ov-file\"> HR-net pose model </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "817a2b50-177e-40d2-a035-5dacaf9225c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully initialized from YAML!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from hrnetpose_model import get_pose_net\n",
    "from configs import load_configs\n",
    "\n",
    "# 1. Load the YAML\n",
    "cfg = load_configs('hrnet_w48_model_configs.yaml')\n",
    "\n",
    "# 2. Initialize Model\n",
    "teacher_model = get_pose_net(cfg, is_train=False)\n",
    "\n",
    "# 3. Load Weights\n",
    "checkpoint = torch.load('hrnet_pose_models/pose_hrnet_w48_384x288.pth', map_location='cpu')\n",
    "teacher_model.load_state_dict(checkpoint)\n",
    "teacher_model.eval()\n",
    "\n",
    "print(\"Model successfully initialized from YAML!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b7a032-2142-4114-bef2-7b4bb1ad553a",
   "metadata": {},
   "source": [
    "#### Student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0c9cb87-ff9f-406b-8cde-2100e6c62ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple\n",
    "\n",
    "# Define the output structure\n",
    "KeypointOutput = namedtuple('KeypointOutput', ['heatmaps'])\n",
    "\n",
    "class SqueezeNetHPE(nn.Module):\n",
    "    def __init__(self, num_keypoints=17):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load SqueezeNet backbone (stride 16)\n",
    "        squeezenet = models.squeezenet1_1(weights=models.SqueezeNet1_1_Weights.IMAGENET1K_V1)\n",
    "        self.backbone = squeezenet.features\n",
    "\n",
    "        # Simple decoder\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # SqueezeNet 1.1 features output 512 channels\n",
    "        self.conv_heatmap = nn.Conv2d(\n",
    "            in_channels=512,\n",
    "            out_channels=num_keypoints,\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Feature extraction\n",
    "        # Input: [B, 3, 384, 288] -> Output: [B, 512, 23, 17]\n",
    "        x = self.backbone(x)\n",
    "\n",
    "        # 2. Decoder\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # 3. Explicit Upsampling \n",
    "        # Instead of scale_factor=4, we provide the exact target dimensions.\n",
    "        # This fixes the \"92x68\" issue by forcing the output to 96x72.\n",
    "        x = F.interpolate(\n",
    "            x,\n",
    "            size=(96, 72), \n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False\n",
    "        )\n",
    "\n",
    "        # 4. Final heatmap convolution\n",
    "        heatmaps = self.conv_heatmap(x)\n",
    "        \n",
    "        # Return as namedtuple\n",
    "        return KeypointOutput(heatmaps=heatmaps)\n",
    "\n",
    "# --- Initialization and Debug ---\n",
    "\n",
    "# 1. Initialize the model\n",
    "num_keypoints = 17\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "student_model = SqueezeNetHPE(num_keypoints=num_keypoints).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93385d5c-66f9-45b7-b12f-9d99943c82d3",
   "metadata": {},
   "source": [
    "#### Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e9994a-f9cf-42ab-86a6-788efbba44ad",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f237b875-b945-4c8f-a036-e1977fe6d236",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import cv2\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pycocotools.coco import COCO  # type: ignore[import-untyped]\n",
    "from typing import Any, Union, List, Tuple, Optional, Callable, cast, Dict, Iterator\n",
    "\n",
    "def find_images_with_coco_keypoints(\n",
    "    ann_file: str, img_root: str, verbose: bool = True\n",
    ") -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Returns a list of image file paths that satisfy:\n",
    "    - At least one person with all 17 keypoints visible, OR\n",
    "    - More than one person, with each having at least 12 visible keypoints.\n",
    "\n",
    "    Args:\n",
    "        ann_file (str): Path to COCO annotation file.\n",
    "        img_root (str): Path to image folder.\n",
    "        verbose (bool): If True, prints info about matches.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of image file paths.\n",
    "    \"\"\"\n",
    "    coco = COCO(ann_file)\n",
    "    selected_with_17_images = []\n",
    "    selected_with_12_more_images = []\n",
    "    for img_id in coco.imgs.keys():\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id, catIds=[1], iscrowd=None)\n",
    "        anns = coco.loadAnns(ann_ids)\n",
    "        keypoints_per_person = []\n",
    "        for ann in anns:\n",
    "            keypoints = ann[\"keypoints\"]\n",
    "            visibility_flags = keypoints[2::3]\n",
    "            num_visible = sum([v == 2 for v in visibility_flags])\n",
    "            keypoints_per_person.append(num_visible)\n",
    "        # Condition 1: at least one person with all 17 keypoints visible\n",
    "        cond1 = any(k == 17 for k in keypoints_per_person)\n",
    "        # Condition 2: more than one person, AND each has at least 12 keypoints visible\n",
    "        cond2 = len(keypoints_per_person) > 1 and all(\n",
    "            k >= 12 for k in keypoints_per_person\n",
    "        )\n",
    "        if cond1:\n",
    "            imginfo = coco.loadImgs(img_id)[0]\n",
    "            file_name = imginfo[\"file_name\"]\n",
    "            img_path = os.path.join(img_root, file_name)\n",
    "            selected_with_17_images.append(img_path)\n",
    "        if cond2:\n",
    "            imginfo = coco.loadImgs(img_id)[0]\n",
    "            file_name = imginfo[\"file_name\"]\n",
    "            img_path = os.path.join(img_root, file_name)\n",
    "            selected_with_12_more_images.append(img_path)\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"\\nTotal images found: \\\n",
    "            {len(selected_with_17_images)} and {len(selected_with_12_more_images)}\"\n",
    "        )\n",
    "    return selected_with_17_images, selected_with_12_more_images\n",
    "\n",
    "\n",
    "def extract_list_keypoints_and_visibility(\n",
    "    keypoints: Union[List[float], List[List[float]]]\n",
    ") -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Extract keypoint coordinates and visibility flags from a\n",
    "    single list or a list of keypoints lists.\n",
    "    \"\"\"\n",
    "\n",
    "    def _process_single_keypoints(kps: List[float]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        if len(kps) % 3 != 0:\n",
    "            raise ValueError(f\"Keypoints length must be multiple of 3, got {len(kps)}\")\n",
    "        coords = np.array(\n",
    "            [[kps[i], kps[i + 1]] for i in range(0, len(kps), 3)], dtype=np.float32\n",
    "        )\n",
    "        visibility = np.array(\n",
    "            [kps[i + 2] for i in range(0, len(kps), 3)], dtype=np.float32\n",
    "        )\n",
    "        return coords, visibility\n",
    "\n",
    "    coords_list, visibility_list = [], []\n",
    "\n",
    "    if not keypoints:\n",
    "        raise ValueError(\"Input keypoints list is empty.\")\n",
    "\n",
    "    if isinstance(keypoints[0], (float, int)):\n",
    "        kps = cast(List[float], keypoints)\n",
    "        coords, visibility = _process_single_keypoints(kps)\n",
    "        coords_list.append(coords)\n",
    "        visibility_list.append(visibility)\n",
    "\n",
    "    elif isinstance(keypoints[0], (list, tuple)):\n",
    "        keypoints_list = cast(List[List[float]], keypoints)\n",
    "        for kp in keypoints_list:\n",
    "            coords, visibility = _process_single_keypoints(kp)\n",
    "            coords_list.append(coords)\n",
    "            visibility_list.append(visibility)\n",
    "    else:\n",
    "        raise TypeError(\"Input must be a list of floats or a list of lists of floats.\")\n",
    "\n",
    "    return coords_list, visibility_list\n",
    "\n",
    "\n",
    "def extract_keypoints_and_visibility(\n",
    "    keypoints: List[float],\n",
    ") -> Tuple[List[List[float]], List[int]]:\n",
    "    \"\"\"\n",
    "    Extracts (x, y) coordinates and visibility flags from a flat keypoints list.\n",
    "\n",
    "    Args:\n",
    "        keypoints (List[float]): A flat list structured as [x1, y1, v1, x2, y2, v2, ...].\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[List[float]], List[int]]:\n",
    "            - coords: list of [x, y] pairs\n",
    "            - visibility: list of visibility flags (0, 1, or 2)\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the keypoints list length is not a multiple of 3.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if len(keypoints) % 3 != 0:\n",
    "            raise ValueError(\n",
    "                \"Keypoints list length must be a multiple of 3 (x, y, v per keypoint).\"\n",
    "            )\n",
    "\n",
    "        coords: List[List[float]] = []\n",
    "        visibility: List[int] = []\n",
    "\n",
    "        for i in range(0, len(keypoints), 3):\n",
    "            x = float(keypoints[i])\n",
    "            y = float(keypoints[i + 1])\n",
    "            v = int(keypoints[i + 2])\n",
    "            coords.append([x, y])\n",
    "            visibility.append(v)\n",
    "\n",
    "        logger.debug(f\"Extracted {len(coords)} keypoints successfully.\")\n",
    "        return coords, visibility\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting keypoints and visibility: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def process_image_and_keypoints(\n",
    "    image: np.ndarray,\n",
    "    keypoints: Union[List[List[float]], np.ndarray],\n",
    "    bbox: Union[List[int], Tuple[int, int, int, int]],\n",
    "    target_size: Tuple[int, int],\n",
    "    angle: float = 0,\n",
    "    flip: bool = False,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Crop, resize, rotate, and optionally flip an image and its corresponding keypoints\n",
    "    for pose estimation tasks.\n",
    "\n",
    "    Steps:\n",
    "        1. Crop the image using the bounding box and clamp to image bounds.\n",
    "        2. Shift keypoints relative to the crop coordinates.\n",
    "        3. Mask keypoints outside the crop.\n",
    "        4. Resize the image and scale keypoints to the target size.\n",
    "        5. Rotate image and keypoints if angle != 0.\n",
    "        6. Flip image horizontally if flip=True.\n",
    "        7. Restore [0, 0] for originally invisible or out-of-bounds keypoints.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): Input image of shape (H, W, C).\n",
    "        keypoints (list or np.ndarray): Keypoints of shape (N, 2) or single keypoint (2,).\n",
    "        bbox (list or tuple): Bounding box [x, y, w, h] for cropping.\n",
    "        target_size (tuple): Desired output size (width, height) after resizing.\n",
    "        angle (float, optional): Rotation angle in degrees. Default is 0.\n",
    "        flip (bool, optional): Whether to horizontally flip the image. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray]:\n",
    "            - resized_image: Processed image of shape (target_height, target_width, C)\n",
    "            - keypoints: Transformed keypoints of shape (N, 2)\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If keypoints array is not in expected shape.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        x1, y1, w, h = map(lambda x: int(round(x)), bbox)\n",
    "        x2, y2 = x1 + w, y1 + h\n",
    "        x1 = max(0, x1)\n",
    "        y1 = max(0, y1)\n",
    "        x2 = min(image.shape[1], x2)\n",
    "        y2 = min(image.shape[0], y2)\n",
    "\n",
    "        # Step 1: Cropping image\n",
    "        cropped_image = image[y1:y2, x1:x2]\n",
    "\n",
    "        # Step 2: Shifting keypoints\n",
    "        kps: np.ndarray = np.asarray(keypoints, dtype=np.float32)\n",
    "        if kps.ndim == 1 and kps.size == 2:\n",
    "            kps = kps.reshape(1, 2)\n",
    "        elif kps.ndim != 2 or kps.shape[1] != 2:\n",
    "            raise ValueError(f\"Keypoints must be of shape [N,2], got shape {kps.shape}\")\n",
    "\n",
    "        zero_mask = np.all(kps == 0, axis=1)\n",
    "        kps -= np.array([x1, y1])\n",
    "\n",
    "        # Step 3: Masking keypoints outside crop\n",
    "        crop_h, crop_w = cropped_image.shape[:2]\n",
    "        invalid_mask = (\n",
    "            (kps[:, 0] < 0)\n",
    "            | (kps[:, 1] < 0)\n",
    "            | (kps[:, 0] >= crop_w)\n",
    "            | (kps[:, 1] >= crop_h)\n",
    "        )\n",
    "\n",
    "        # Step 4: Resizing the image and scale keypoints\n",
    "        resized_image = cv2.resize(cropped_image, target_size)\n",
    "        scale_x = target_size[0] / crop_w\n",
    "        scale_y = target_size[1] / crop_h\n",
    "        kps *= np.array([scale_x, scale_y])\n",
    "\n",
    "        # Step 5: Rotating image and keypoints\n",
    "        if angle != 0:\n",
    "            center = (target_size[0] // 2, target_size[1] // 2)\n",
    "            rot_mat = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "            resized_image = cv2.warpAffine(resized_image, rot_mat, target_size)\n",
    "            kps = (rot_mat[:, :2] @ kps.T + rot_mat[:, 2:]).T\n",
    "\n",
    "        # Step 6: Flipping horizontally when required\n",
    "        if flip:\n",
    "            resized_image = cv2.flip(resized_image, 1)\n",
    "            kps[:, 0] = target_size[0] - kps[:, 0]\n",
    "\n",
    "        # Step 7: Restoring [0,0] for invisible or out-of-bounds keypoints\n",
    "        kps[zero_mask | invalid_mask] = [0, 0]\n",
    "\n",
    "        logger.debug(\n",
    "            f\"Processed image shape: {resized_image.shape}, keypoints shape: {kps.shape}\"\n",
    "        )\n",
    "        return resized_image, kps\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in process_image_and_keypoints: {e}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "def process_image(\n",
    "    image: np.ndarray,\n",
    "    bbox: list[float] | tuple[float, float, float, float],\n",
    "    target_size: tuple[int, int],\n",
    "    angle: float = 0,\n",
    "    flip: bool = False,\n",
    ") -> tuple[np.ndarray, bool]:\n",
    "    \"\"\"\n",
    "    Processes an image for pose estimation inference using a bounding box.\n",
    "\n",
    "    This function crops, resizes, and optionally rotates or flips an image\n",
    "    based on a given bounding box, preparing it for pose estimation input.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): Input image in BGR format (as read by OpenCV).\n",
    "        bbox (list[float] | tuple[float, float, float, float]): Bounding box in\n",
    "            COCO format [x, y, width, height].\n",
    "        target_size (tuple[int, int]): Target output size as (width, height).\n",
    "        angle (float, optional): Rotation angle in degrees. Defaults to 0.\n",
    "        flip (bool, optional): If True, flip the image horizontally. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, bool]:\n",
    "            - Processed image (np.ndarray).\n",
    "            - Success flag (bool), True if processing succeeded, False otherwise.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the bounding box dimensions are invalid or out of image bounds.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if image is None or not isinstance(image, np.ndarray):\n",
    "            raise ValueError(\"Invalid image input: must be a numpy array.\")\n",
    "\n",
    "        # Convert bbox and clamp to image bounds\n",
    "        x1, y1, x2, y2 = map(lambda x: int(round(x)), bbox)\n",
    "        w = x2 - x1\n",
    "        h = y2 - y1\n",
    "\n",
    "        # Ensure bbox is within image dimensions\n",
    "        if w <= 0 or h <= 0:\n",
    "            raise ValueError(f\"Invalid bbox dimensions: {bbox}\")\n",
    "\n",
    "        x1 = max(0, x1)\n",
    "        y1 = max(0, y1)\n",
    "        x2 = min(image.shape[1], x2)\n",
    "        y2 = min(image.shape[0], y2)\n",
    "\n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            raise ValueError(\n",
    "                f\"Invalid bbox coordinates after clamping: {(x1, y1, x2, y2)}\"\n",
    "            )\n",
    "\n",
    "        # Step 1: Crop image\n",
    "        cropped_image = image[y1:y2, x1:x2]\n",
    "\n",
    "        # Step 2: Resize image\n",
    "        resized_image = cv2.resize(cropped_image, target_size)\n",
    "\n",
    "        # Step 3: Rotate if needed\n",
    "        if angle != 0:\n",
    "            center = (target_size[0] // 2, target_size[1] // 2)\n",
    "            rot_mat = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "            resized_image = cv2.warpAffine(resized_image, rot_mat, target_size)\n",
    "\n",
    "        # Step 4: Flip horizontally if needed\n",
    "        if flip:\n",
    "            resized_image = cv2.flip(resized_image, 1)\n",
    "\n",
    "        return resized_image, True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[process_image] Error processing image: {e}\")\n",
    "        blank_image = np.zeros((target_size[1], target_size[0], 3), dtype=np.uint8)\n",
    "        return blank_image, False\n",
    "\n",
    "\n",
    "def generate_heatmaps(\n",
    "    keypoints: np.ndarray,\n",
    "    keypoints_visible: np.ndarray,\n",
    "    input_size: Tuple[int, int],\n",
    "    heatmap_size: Tuple[int, int] = (64, 48),\n",
    "    sigma: float = 2.0,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Generate Gaussian heatmaps centered at keypoints for pose estimation,\n",
    "    and return a mask for visible/labeled keypoints.\n",
    "\n",
    "    Args:\n",
    "        keypoints (np.ndarray): Keypoint coordinates in input image space, shape (K, 2)\n",
    "        keypoints_visible (np.ndarray): Visibility flags for each keypoint, shape (K,)\n",
    "        input_size (Tuple[int, int]): Input image size (width, height)\n",
    "        heatmap_size (Tuple[int, int], optional): Output heatmap size (width, height).\n",
    "        Default is (64, 48)\n",
    "        sigma (float, optional): Gaussian sigma controlling spread. Default is 2.0\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]:\n",
    "            - Heatmaps tensor of shape (K, H, W)\n",
    "            - Mask tensor of shape (K,) with 1 for valid keypoints, 0 for missing/invisible\n",
    "    \"\"\"\n",
    "    try:\n",
    "        K, D = keypoints.shape\n",
    "        if D != 2:\n",
    "            raise ValueError(\n",
    "                f\"Keypoints should have shape (K, 2), got {keypoints.shape}\"\n",
    "            )\n",
    "\n",
    "        W, H = heatmap_size\n",
    "        w, h = input_size\n",
    "\n",
    "        heatmaps = np.zeros((K, H, W), dtype=np.float32)\n",
    "        mask = np.zeros((K,), dtype=np.float32)\n",
    "\n",
    "        # Scale factor from input image to heatmap\n",
    "        scale_factor = np.array([(W - 1) / (w - 1), (H - 1) / (h - 1)])\n",
    "\n",
    "        for k in range(K):\n",
    "            if keypoints_visible[k] < 0.5:\n",
    "                continue\n",
    "\n",
    "            x, y = keypoints[k]\n",
    "            x_hm = x * scale_factor[0]\n",
    "            y_hm = y * scale_factor[1]\n",
    "\n",
    "            if not (0 <= x_hm < W and 0 <= y_hm < H):\n",
    "                continue\n",
    "\n",
    "            # Creating Gaussian heatmap\n",
    "            x_grid = np.arange(W)\n",
    "            y_grid = np.arange(H)\n",
    "            xx, yy = np.meshgrid(x_grid, y_grid)\n",
    "            gaussian = np.exp(-((xx - x_hm) ** 2 + (yy - y_hm) ** 2) / (2 * sigma**2))\n",
    "            heatmaps[k] = gaussian\n",
    "            mask[k] = 1.0\n",
    "\n",
    "        heatmaps_tensor = torch.from_numpy(heatmaps)\n",
    "        mask_tensor = torch.from_numpy(mask)\n",
    "\n",
    "        return heatmaps_tensor, mask_tensor\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in generate_heatmaps: {e}\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1109712-93ca-4ca7-a4fa-72ce538c6d60",
   "metadata": {},
   "source": [
    "#### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74f5300e-23a7-4c48-bf0f-95f8aba820ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=5.76s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=5.68s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 13:25:16,380 - INFO - Mapping all valid person annotations...\n",
      "2026-01-29 13:25:16,500 - INFO - Dataset initialized: 38151 people found across 30000 images.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.23s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 13:25:17,031 - INFO - Mapping all valid person annotations...\n",
      "2026-01-29 13:25:17,066 - INFO - Dataset initialized: 6352 people found across 5000 images.\n",
      "2026-01-29 13:25:17,067 - INFO - COCO DataLoaders created successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.22s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pycocotools.coco import COCO\n",
    "from typing import Optional, Callable, Any, List, Tuple\n",
    "from torch import Tensor\n",
    "\n",
    "# Setting up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_downloaded_image_ids(root_dir: str, ann_file: str) -> List[int]:\n",
    "    \"\"\"\n",
    "    Checks the local directory and returns IDs of images that actually exist.\n",
    "    \"\"\"\n",
    "    coco = COCO(ann_file)\n",
    "    all_img_ids = coco.getImgIds()\n",
    "    existing_ids = []\n",
    "    \n",
    "    # Fast check: see if the file exists on disk\n",
    "    for img_id in all_img_ids:\n",
    "        filename = coco.loadImgs(img_id)[0]['file_name']\n",
    "        if os.path.exists(os.path.join(root_dir, filename)):\n",
    "            existing_ids.append(img_id)\n",
    "            \n",
    "    return existing_ids\n",
    "\n",
    "class CustomCocoKeypoints(Dataset):\n",
    "    \"\"\"\n",
    "    Custom COCO keypoints dataset where each item is a single person annotation.\n",
    "    Instead of 1 image = 1 sample, we use 1 person = 1 sample.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, root: str, annFile: str, transform: Optional[Callable] = None\n",
    "    ) -> None:\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load COCO API\n",
    "        self.coco = COCO(annFile)\n",
    "        \n",
    "        # FIX: Ensure arguments match the utility function signature (root_dir, ann_file)\n",
    "        try:\n",
    "            downloaded_ids = get_downloaded_image_ids(self.root, annFile)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not filter downloaded images: {e}. Using all IDs from JSON.\")\n",
    "            downloaded_ids = list(self.coco.imgs.keys())\n",
    "\n",
    "        logger.info(\"Mapping all valid person annotations...\")\n",
    "        \n",
    "        # Store pairs of (image_id, annotation_dict)\n",
    "        self.valid_anns = []\n",
    "        for img_id in downloaded_ids:\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "            \n",
    "            # Filter for annotations that actually have keypoints\n",
    "            for ann in anns:\n",
    "                if ann.get(\"num_keypoints\", 0) > 0:\n",
    "                    # Every person becomes a distinct entry in the dataset\n",
    "                    self.valid_anns.append((img_id, ann))\n",
    "        \n",
    "        logger.info(\n",
    "            f\"Dataset initialized: {len(self.valid_anns)} people found \"\n",
    "            f\"across {len(downloaded_ids)} images.\"\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.valid_anns)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "        try:\n",
    "            # 1. Get image ID and specific annotation for this index\n",
    "            img_id, target = self.valid_anns[index]\n",
    "            \n",
    "            # 2. Load Image\n",
    "            img_info = self.coco.loadImgs(img_id)[0]\n",
    "            img_path = os.path.join(self.root, img_info[\"file_name\"])\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            num_image = np.array(img)\n",
    "\n",
    "            # 3. Extract keypoints and bounding box for THIS specific person\n",
    "            bbox = target[\"bbox\"]\n",
    "        \n",
    "            \n",
    "            keyp, visibility = extract_keypoints_and_visibility(target[\"keypoints\"])\n",
    "            \n",
    "            # 4. Process (Crop to person bbox, resize, etc.)\n",
    "            processed_img_np, processed_keypoints = process_image_and_keypoints(\n",
    "                image=num_image,\n",
    "                keypoints=keyp,\n",
    "                bbox=bbox,\n",
    "                target_size=(288, 384),\n",
    "                angle=0,\n",
    "                flip=False,\n",
    "            )\n",
    "\n",
    "            # 5. Convert Image to Tensor [3, H, W]\n",
    "            processed_img: Tensor = (\n",
    "                torch.from_numpy(processed_img_np).permute(2, 0, 1).float() / 255.0\n",
    "            )\n",
    "\n",
    "            # 6. Generate Heatmaps\n",
    "            input_size = (384, 288)\n",
    "            heatmap_size = (96, 72)\n",
    "            visibility_arr = np.asarray(visibility, dtype=np.float32)\n",
    "            \n",
    "            heatmaps_tensor, _mask_tensor = generate_heatmaps(\n",
    "                keypoints=processed_keypoints,\n",
    "                keypoints_visible=visibility_arr,\n",
    "                input_size=input_size,\n",
    "                heatmap_size=heatmap_size,\n",
    "                sigma=2.0,\n",
    "            )\n",
    "\n",
    "            return processed_img, heatmaps_tensor, _mask_tensor\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing index {index}: {e}\")\n",
    "            raise e\n",
    "\n",
    "def collate_fn(\n",
    "    batch: List[Tuple[Tensor, Tensor, Tensor]]\n",
    ") -> Tuple[Tensor, Tensor, Tensor]:\n",
    "    processed_img, heatmap_tensor, _mask_tensor = zip(*batch)\n",
    "    return (\n",
    "        torch.stack(processed_img),\n",
    "        torch.stack(heatmap_tensor),\n",
    "        torch.stack(_mask_tensor)\n",
    "    )\n",
    "\n",
    "def get_coco_dataloaders(\n",
    "    root_train: str, ann_train: str,\n",
    "    root_val: str, ann_val: str,\n",
    "    batch_size: int = 32, num_workers: int = 4,\n",
    "    pin_memory: bool = True, persistent_workers: bool = True,\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "    \n",
    "    try:\n",
    "        train_dataset = CustomCocoKeypoints(root=root_train, annFile=ann_train)\n",
    "        val_dataset = CustomCocoKeypoints(root=root_val, annFile=ann_val)\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            persistent_workers=persistent_workers,\n",
    "            collate_fn=collate_fn,\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            dataset=val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            persistent_workers=persistent_workers,\n",
    "            collate_fn=collate_fn,\n",
    "            drop_last=True,\n",
    "        )\n",
    "        \n",
    "        logger.info(\"COCO DataLoaders created successfully.\")\n",
    "        return train_loader, val_loader\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating COCO dataloaders: {e}\")\n",
    "        raise e\n",
    "\n",
    "# --- Execution ---\n",
    "root_train = \"/home/clinton-mwangi/fiftyone/coco-2017/train/data\"\n",
    "root_val = \"/home/clinton-mwangi/fiftyone/coco-2017/validation/data\"\n",
    "ann_train = \"/home/clinton-mwangi/fiftyone/coco-2017/raw/person_keypoints_train2017.json\"\n",
    "ann_val = \"/home/clinton-mwangi/fiftyone/coco-2017/raw/person_keypoints_val2017.json\"\n",
    "\n",
    "train_loader, val_loader = get_coco_dataloaders(\n",
    "    root_train=root_train,\n",
    "    ann_train=ann_train,\n",
    "    root_val=root_val,\n",
    "    ann_val=ann_val,\n",
    "    batch_size=8,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6360ff9f-1590-4e00-85c9-872102fe1a2e",
   "metadata": {},
   "source": [
    "#### LOad the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4fa21ba-0f69-4d73-a359-9f871e9a0137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Module\n",
    "class AdaptiveWingLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Adaptive Wing Loss for keypoint heatmap regression.\n",
    "\n",
    "    This loss is designed for heatmap-based keypoint estimation tasks,\n",
    "    giving higher weight to small but important errors, and less weight\n",
    "    to large errors that might correspond to outliers.\n",
    "\n",
    "    Args:\n",
    "        omega (float): Maximum weight for the loss. Default: 14\n",
    "        theta (float): Threshold separating small and large errors. Default: 0.5\n",
    "        epsilon (float): Small constant for numerical stability. Default: 1\n",
    "        alpha (float): Exponent controlling sensitivity to small errors. Default: 2.1\n",
    "        weight (float): Additional weight for keypoints above a threshold. Default: 10\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        omega: float = 14,\n",
    "        theta: float = 0.5,\n",
    "        epsilon: float = 1.0,\n",
    "        alpha: float = 2.1,\n",
    "        weight: float = 10.0,\n",
    "    ):\n",
    "        super(AdaptiveWingLoss, self).__init__()\n",
    "        self.omega = omega\n",
    "        self.theta = theta\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.weight = weight\n",
    "        logger.info(\n",
    "            f\"AdaptiveWingLoss initialized with omega={omega},\\\n",
    "              theta={theta}, epsilon={epsilon}, alpha={alpha}, weight={weight}\"\n",
    "        )\n",
    "\n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the Adaptive Wing Loss between predicted and target heatmaps.\n",
    "\n",
    "        Args:\n",
    "            pred (torch.Tensor): Predicted heatmaps of shape (B, N, H, W)\n",
    "            target (torch.Tensor): Ground-truth heatmaps of shape (B, N, H, W)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Scalar loss value\n",
    "        \"\"\"\n",
    "        try:\n",
    "            delta = torch.abs(pred - target)\n",
    "\n",
    "            # Dilated heatmap using 3x3 max pooling\n",
    "            Hd = F.max_pool2d(target, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "            # Mask where dilated heatmap exceeds threshold\n",
    "            M = (Hd >= 0.2).float()\n",
    "\n",
    "            # Constants for the adaptive wing function\n",
    "            A = (\n",
    "                self.omega\n",
    "                * (\n",
    "                    1.0\n",
    "                    / (1.0 + torch.pow(self.theta / self.epsilon, self.alpha - target))\n",
    "                )\n",
    "                * (self.alpha - target)\n",
    "                * (torch.pow(self.theta / self.epsilon, self.alpha - target - 1))\n",
    "                * (1.0 / self.epsilon)\n",
    "            )\n",
    "\n",
    "            C = self.theta * A - self.omega * torch.log(\n",
    "                1.0 + torch.pow(self.theta / self.epsilon, self.alpha - target)\n",
    "            )\n",
    "\n",
    "            # Apply piecewise loss\n",
    "            losses = torch.where(\n",
    "                delta < self.theta,\n",
    "                self.omega\n",
    "                * torch.log(1.0 + torch.pow(delta / self.epsilon, self.alpha - target)),\n",
    "                A * delta - C,\n",
    "            )\n",
    "\n",
    "            # Apply weighted loss map\n",
    "            weighted_losses = losses * (self.weight * M + 1)\n",
    "\n",
    "            loss_mean = weighted_losses.mean()\n",
    "            logger.debug(\n",
    "                f\"AdaptiveWingLoss computed, mean loss: {loss_mean.item():.6f}\"\n",
    "            )\n",
    "\n",
    "            return loss_mean\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in AdaptiveWingLoss forward pass: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "669800bf-b7d5-4af8-8471-17d9df351edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea647c7-45e6-47ef-ad9a-3404e9baadfa",
   "metadata": {},
   "source": [
    "#### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6eb0a73-a16d-4bc9-a2d1-b0cf2a906dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for imgs, gt_heatmaps, mask in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "537058e5-93ff-4912-abc5-0970625b6434",
   "metadata": {},
   "outputs": [],
   "source": [
    "ht = teacher_model(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4cedfab-0e01-49cd-a888-244d2b0e833a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 17, 96, 72])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c67e4fb-6c1f-452b-9521-bc5954e81638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 384, 288])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29686e76-c235-4686-b8f0-e3205c975b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = student_model(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ac2b3f7-6139-44e6-adfe-b1d642789879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 17, 96, 72])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.heatmaps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2867a4bd-a8cd-4f82-aa5a-a4f6747390df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm  # type: ignore[import-untyped]\n",
    "from torch.amp import GradScaler\n",
    "from collections import namedtuple\n",
    "from copy import deepcopy\n",
    "import psutil\n",
    "epochs = 1\n",
    "loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "for batch_idx, (imgs, gt_heatmaps, masks) in enumerate(loop):\n",
    "    imgs = imgs.to(device, non_blocking=True)\n",
    "    heatmaps = gt_heatmaps.to(device, non_blocking=True)\n",
    "    masks = masks.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f17fce0b-9ebb-4702-88f4-c8d718a5938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm  # type: ignore[import-untyped]\n",
    "from torch.amp import GradScaler\n",
    "from collections import namedtuple\n",
    "from copy import deepcopy\n",
    "import psutil  # type: ignore[import-untyped]\n",
    "def train_student_with_teacher(\n",
    "    student_model: nn.Module,\n",
    "    teacher_model: nn.Module,\n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    val_loader: torch.utils.data.DataLoader,\n",
    "    epochs: int = 3,\n",
    "    lr: float = 5e-4,\n",
    "    alpha: float = 0.7,\n",
    "    weight_decay: float = 1e-5,\n",
    "    val_frequency: int = 5,\n",
    "    save_dir: str = \"./models\",\n",
    ") -> tuple[nn.Module, dict]:\n",
    "    \"\"\"\n",
    "    Train a student model using a teacher model for knowledge distillation.\n",
    "\n",
    "    Args:\n",
    "        student_model (nn.Module): Student model to train.\n",
    "        teacher_model (nn.Module): Teacher model providing target heatmaps.\n",
    "        train_loader (DataLoader): Training dataset loader.\n",
    "        val_loader (DataLoader): Validation dataset loader.\n",
    "        epochs (int, optional): Number of training epochs. Default: 3\n",
    "        lr (float, optional): Learning rate. Default: 5e-4\n",
    "        alpha (float, optional): Weight for distillation loss. Default: 0.7\n",
    "        weight_decay (float, optional): Weight decay for optimizer. Default: 1e-5\n",
    "        val_frequency (int, optional): Frequency (in epochs) to run validation. Default: 5\n",
    "        save_dir (str, optional): Directory to save best model. Default: \"./models\"\n",
    "\n",
    "    Returns:\n",
    "        Tuple[nn.Module, dict]: Trained student model and training history\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        student_model.to(device)\n",
    "        teacher_model.to(device)\n",
    "        teacher_model.eval()\n",
    "\n",
    "        for param in teacher_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        optimizer = optim.AdamW(\n",
    "            student_model.parameters(), lr=lr, weight_decay=weight_decay\n",
    "        )\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "            optimizer, milestones=[20, 27], gamma=0.1\n",
    "        )\n",
    "        criterion = AdaptiveWingLoss()\n",
    "\n",
    "        scaler = GradScaler(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        history: Dict[str, List[float]] = {\"train_loss\": [], \"val_loss\": [], \"lr\": []}\n",
    "        best_val_loss = float(\"inf\")\n",
    "        best_model_state = None\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            student_model.train()\n",
    "            train_loss = 0.0\n",
    "            loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "            for batch_idx, (imgs, gt_heatmaps, masks) in enumerate(loop):\n",
    "                imgs = imgs.to(device, non_blocking=True)\n",
    "                heatmaps = gt_heatmaps.to(device, non_blocking=True)\n",
    "                masks = masks.to(device, non_blocking=True)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                with torch.no_grad(), torch.amp.autocast(device_type=device.type):\n",
    "                    dataset_index = torch.zeros(\n",
    "                        imgs.size(0), dtype=torch.long, device=imgs.device\n",
    "                    )\n",
    "                    teacher_heatmaps = teacher_model(\n",
    "                        imgs\n",
    "                    )\n",
    "\n",
    "                with torch.amp.autocast(device_type=device.type):\n",
    "                    student_heatmaps = student_model(imgs).heatmaps\n",
    "                    mask_expanded = masks.unsqueeze(-1).unsqueeze(-1)\n",
    "                    student_loss = criterion(\n",
    "                        student_heatmaps * mask_expanded, heatmaps * mask_expanded\n",
    "                    )\n",
    "                    distillation_loss = criterion(\n",
    "                        student_heatmaps * mask_expanded,\n",
    "                        teacher_heatmaps * mask_expanded,\n",
    "                    )\n",
    "                    total_loss = alpha * distillation_loss + (1 - alpha) * student_loss\n",
    "\n",
    "                scaler.scale(total_loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(student_model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                train_loss += total_loss.item()\n",
    "                loop.set_postfix({\"loss\": train_loss / (batch_idx + 1)})\n",
    "\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            history[\"train_loss\"].append(avg_train_loss)\n",
    "            history[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n",
    "            scheduler.step()\n",
    "            logger.info(f\"Epoch {epoch+1} - Avg training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "            if (epoch + 1) % val_frequency == 0 or (epoch + 1) == epochs:\n",
    "                val_loss = validate_student(\n",
    "                    student_model, teacher_model, val_loader, criterion, alpha\n",
    "                )\n",
    "                history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "                logger.info(f\"Epoch {epoch+1} - Validation loss: {val_loss:.4f}\")\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    model_to_save = student_model\n",
    "                    best_model_state = model_to_save.state_dict()\n",
    "                    torch.save(\n",
    "                        best_model_state,\n",
    "                        os.path.join(save_dir, \"best_student_model.pth\"),\n",
    "                    )\n",
    "                    logger.info(f\"Saved new best model (val_loss={val_loss:.4f})\")\n",
    "\n",
    "            # Saving chechpoint at every 5 epoch\n",
    "            if (epoch + 1) % 5 == 0 or (epoch + 1) == epochs:\n",
    "                model_to_save = (\n",
    "                    student_model.module\n",
    "                    if isinstance(student_model, nn.DataParallel)\n",
    "                    else student_model\n",
    "                )\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"epoch\": epoch + 1,\n",
    "                        \"model_state_dict\": model_to_save.state_dict(),\n",
    "                        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                        \"loss\": avg_train_loss,\n",
    "                    },\n",
    "                    os.path.join(save_dir, f\"checkpoint_epoch_{epoch+1}.pth\"),\n",
    "                )\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        # Load best model\n",
    "        if best_model_state:\n",
    "            model_to_load = (\n",
    "                student_model.module\n",
    "                if isinstance(student_model, nn.DataParallel)\n",
    "                else student_model\n",
    "            )\n",
    "            model_to_load.load_state_dict(best_model_state)\n",
    "            logger.info(f\"Loaded best model with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "        return student_model, history\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during training: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "257e23fd-be8d-4a88-ac69-036c16748928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 13:37:07,685 - INFO - AdaptiveWingLoss initialized with omega=14,              theta=0.5, epsilon=1.0, alpha=2.1, weight=10.0\n",
      "Epoch 1/3:   0%|                                       | 0/4769 [00:00<?, ?it/s]\n",
      "2026-01-29 13:37:07,697 - ERROR - Error during training: DataLoader worker (pid(s) 53512, 53513, 53514, 53515) exited unexpectedly\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 53512, 53513, 53514, 53515) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/2d-human-pose-estimation-using-kd/kd_hpe_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1310\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1309\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1310\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/queues.py:122\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# unserialize the data after having released the lock\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ForkingPickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/2d-human-pose-estimation-using-kd/kd_hpe_env/lib/python3.12/site-packages/torch/multiprocessing/reductions.py:540\u001b[39m, in \u001b[36mrebuild_storage_fd\u001b[39m\u001b[34m(cls, df, size)\u001b[39m\n\u001b[32m    539\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrebuild_storage_fd\u001b[39m(\u001b[38;5;28mcls\u001b[39m, df, size):\n\u001b[32m--> \u001b[39m\u001b[32m540\u001b[39m     fd = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    541\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/resource_sharer.py:57\u001b[39m, in \u001b[36mDupFd.detach\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''Get the fd.  This should only be called once.'''\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_resource_sharer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m reduction.recv_handle(conn)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/resource_sharer.py:86\u001b[39m, in \u001b[36m_ResourceSharer.get_connection\u001b[39m\u001b[34m(ident)\u001b[39m\n\u001b[32m     85\u001b[39m address, key = ident\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m c = \u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthkey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mauthkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m c.send((key, os.getpid()))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/connection.py:519\u001b[39m, in \u001b[36mClient\u001b[39m\u001b[34m(address, family, authkey)\u001b[39m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m     c = \u001b[43mSocketClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m authkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(authkey, \u001b[38;5;28mbytes\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/connection.py:647\u001b[39m, in \u001b[36mSocketClient\u001b[39m\u001b[34m(address)\u001b[39m\n\u001b[32m    646\u001b[39m s.setblocking(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43ms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Connection(s.detach())\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_student_with_teacher\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstudent_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstudent_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 159\u001b[39m, in \u001b[36mtrain_student_with_teacher\u001b[39m\u001b[34m(student_model, teacher_model, train_loader, val_loader, epochs, lr, alpha, weight_decay, val_frequency, save_dir)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    158\u001b[39m     logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError during training: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36mtrain_student_with_teacher\u001b[39m\u001b[34m(student_model, teacher_model, train_loader, val_loader, epochs, lr, alpha, weight_decay, val_frequency, save_dir)\u001b[39m\n\u001b[32m     63\u001b[39m train_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m     64\u001b[39m loop = tqdm(train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_heatmaps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheatmaps\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_heatmaps\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/2d-human-pose-estimation-using-kd/kd_hpe_env/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/2d-human-pose-estimation-using-kd/kd_hpe_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:497\u001b[39m, in \u001b[36mDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    495\u001b[39m         \u001b[38;5;28mself\u001b[39m._iterator = \u001b[38;5;28mself\u001b[39m._get_iterator()\n\u001b[32m    496\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterator\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/2d-human-pose-estimation-using-kd/kd_hpe_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1286\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._reset\u001b[39m\u001b[34m(self, loader, first_iter)\u001b[39m\n\u001b[32m   1284\u001b[39m resume_iteration_cnt = \u001b[38;5;28mself\u001b[39m._num_workers\n\u001b[32m   1285\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m resume_iteration_cnt > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1286\u001b[39m     return_idx, return_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_idx, _utils.worker._ResumeIteration):\n\u001b[32m   1288\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m return_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/2d-human-pose-estimation-using-kd/kd_hpe_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1483\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1479\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1480\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1482\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1483\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1484\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1485\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/2d-human-pose-estimation-using-kd/kd_hpe_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1323\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1321\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) > \u001b[32m0\u001b[39m:\n\u001b[32m   1322\u001b[39m     pids_str = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(w.pid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[32m-> \u001b[39m\u001b[32m1323\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1324\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) exited unexpectedly\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1325\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue.Empty):\n\u001b[32m   1327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: DataLoader worker (pid(s) 53512, 53513, 53514, 53515) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "train_student_with_teacher(\n",
    "    student_model=student_model,\n",
    "    teacher_model=teacher_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3808376b-ca72-4fa9-a7c3-7a5ef9cc51f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(KD-venv)",
   "language": "python",
   "name": "kd_hpe_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
