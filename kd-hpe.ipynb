{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e3961cd-1d0b-42e8-b020-25d13b81c3c0",
   "metadata": {},
   "source": [
    "# Knowledge Distillation for 2D Human Pose Estimation\n",
    "\n",
    "## What is Knowledge Distillation?\n",
    "Knowledge Distillation (KD) is a technique in deep learning where a small, compact model (the **Student**) is trained to reproduce the behavior of a large, complex model (the **Teacher**), or an ensemble of models. The core idea was popularized by Hinton et al. in [\"Distilling the Knowledge in a Neural Network\" (2015)](https://arxiv.org/abs/1503.02531).\n",
    "\n",
    "## Why use KD?\n",
    "1. **Model Compression**: Reduce model size and inference latency.\n",
    "2. **Accuracy Boost**: The Student often learns better from the Teacher's \"soft targets\" (probability distributions or heatmaps) than from one-hot ground truth labels alone, as the Teacher provides structural information about the data.\n",
    "\n",
    "## Types of Knowledge Distillation covered here:\n",
    "1. **Response-based (Logits) KD**: The Student mimics the final output (heatmaps) of the Teacher.\n",
    "2. **Feature-based KD**: The Student mimics the intermediate feature maps of the Teacher, learning to extract similar spatial features.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52912d4b-0cbe-4d74-8451-ea941c7e96b1",
   "metadata": {
    "id": "52912d4b-0cbe-4d74-8451-ea941c7e96b1"
   },
   "source": [
    "# 1. Introduction.\n",
    "\n",
    "In Human Pose Estimation (HPE), the Teacher is typically a high-performance model (like HRNet-W48) that is accurate but computationally expensive. The Student is a lightweight model (like SqueezeNet or MobileNet) designed for real-time inference on edge devices.\n",
    "\n",
    "Nowadays, **Gaussian Heatmaps** are the standard representation for 2D Pose Estimation. Instead of regressing exact (x, y) coordinates directly (which is hard to learn), the model predicts a probability map where the peak represents the keypoint location.\n",
    "\n",
    "**Generation Formula:**\n",
    "For a keypoint $k$ at $(x_k, y_k)$, the value at pixel $(i, j)$ is:\n",
    "\n",
    "$$H_k(i, j) = \\exp\\left( -\\frac{(i - x_k)^2 + (j - y_k)^2}{2\\sigma^2} \\right)$$\n",
    "\n",
    "Where $\\sigma$ controls the spread of the Gaussian peak.\n",
    "\n",
    "**Masks (Visibility):**\n",
    "We also associate a binary **Mask** with each keypoint. This mask is **1** if the keypoint is visible and annotated, and **0** otherwise. During training, we multiply the loss by this mask to essentially 'switch off' the learning for missing or invisible joints, preventing the model from being confused by unlabelled data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f38ea0b0-82a6-44d4-afe2-83b0c5f7d2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clinton-mwangi/Desktop/projects/2d-human-pose-estimation-using-kd/kd_hpe_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Union, Dict\n",
    "import cv2\n",
    "import logging\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pycocotools.coco import COCO\n",
    "from torch import Tensor\n",
    "from typing import Optional, Callable\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "from ptflops import get_model_complexity_info\n",
    "import heatmaps_to_keypoints\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from typing import List, Union, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2aa767-4583-4fba-a59f-1a01988b981f",
   "metadata": {
    "id": "ac0f5f71-b364-4a64-90ee-510b04c08d00"
   },
   "source": [
    "#### Student model\n",
    "\n",
    "We design a lightweight model with <a href=\"https://arxiv.org/pdf/1602.07360\">SquuezeNet backbone</a> pretrained to extarct features from the an images. WSo we just remove tthe classification tail of the model and add a custom backbone for generating heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8bc2b59-20b3-4ead-b902-f59b6cd72cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "KeypointOutput = namedtuple('KeypointOutput', ['heatmaps'])\n",
    "class SqueezeNetHPE(nn.Module):\n",
    "    def __init__(self, num_keypoints=17):\n",
    "        super().__init__()\n",
    "        squeezenet = models.squeezenet1_1(weights=models.SqueezeNet1_1_Weights.IMAGENET1K_V1)\n",
    "        self.backbone = squeezenet.features\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv_heatmap = nn.Conv2d(\n",
    "            in_channels=512,\n",
    "            out_channels=num_keypoints,\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.relu(x)\n",
    "        x = F.interpolate(\n",
    "            x,\n",
    "            size=(96, 72), \n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False\n",
    "        )\n",
    "        heatmaps = self.conv_heatmap(x)\n",
    "\n",
    "        return KeypointOutput(heatmaps=heatmaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43a8307-241a-450c-b3a5-25eabecc929d",
   "metadata": {},
   "source": [
    "1.  **Backbone (`self.backbone`)**: We use the feature extractor from `squeezenet1_1` pretrained on ImageNet. <br>\n",
    "2.  **Upsampling**: SqueezeNet significantly downsamples the input (typically by 32x). To generate high-quality heatmaps (which require spatial precision), we use `F.interpolate` to upsample the features to **96x72**.\n",
    "3.  **Heatmap Head (`self.conv_heatmap`)**: Instead of a Fully Connected layer for classification, we use a final **Convolutional Layer** (kernel size 3x3) to produce **17 output channels**, each corresponding to a keypoint heatmap.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f96ebe4-39ed-4918-b5ac-c05eece46a61",
   "metadata": {},
   "source": [
    "#### Teacher model\n",
    "\n",
    "\n",
    "In this tutprial we will use <a href=\"https://github.com/HRNet/HRNet-Human-Pose-Estimation?tab=readme-ov-file\"> HRNet-pose as a teacher model </a>.\n",
    "\n",
    "**HRNet** (High-Resolution Net) maintains high-resolution representations throughout the network. Unlike traditional architectures (ResNet, VGG) that downsample the image and then upsample, HRNet connects high-to-low resolution subnetworks in parallel.\n",
    "\n",
    "\n",
    "**Role as Teacher:**\n",
    "HRNet-pose is a state-of-the-art CNN based model for 2D Pose Estimation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc40104a-fc60-426a-855c-46c1c7f7912f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hrnet_pose.hrnetpose_model import get_pose_net\n",
    "from hrnet_pose.configs import load_configs\n",
    "\n",
    "cfg = load_configs('hrnet_pose/hrnet_w48_model_configs.yaml')\n",
    "teacher_model = get_pose_net(cfg, is_train=False)\n",
    "checkpoint = torch.load('hrnet_pose/hrnet_pose_models/pose_hrnet_w48_384x288.pth', map_location='cpu')\n",
    "teacher_model.load_state_dict(checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07f4e33-86e8-4556-83e8-5f0996fbb64f",
   "metadata": {
    "id": "e07f4e33-86e8-4556-83e8-5f0996fbb64f"
   },
   "source": [
    "# 2. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5e6a66-a521-4def-ae84-c9a39bed1b89",
   "metadata": {},
   "source": [
    "## 2.1 COCO Keypoint Dataset\n",
    "The COCO (Common Objects in Context) dataset is a large-scale object detection, segmentation, and captioning dataset. <br>\n",
    "For Human Pose Estimation, we use the **Keypoints** task, which annotates **17 keypoints** (joints) on human bodies.\n",
    "\n",
    "The 17 Keypoints are:\n",
    "0: Nose, 1: Left Eye, 2: Right Eye, 3: Left Ear, 4: Right Ear, 5: Left Shoulder, 6: Right Shoulder, 7: Left Elbow, 8: Right Elbow, 9: Left Wrist, 10: Right Wrist, 11: Left Hip, 12: Right Hip, 13: Left Knee, 14: Right Knee, 15: Left Ankle, 16: Right Ankle. <br>\n",
    "\n",
    "The keypoints are connected to form a skeleton structure, helping in visualizing pose.\n",
    "\n",
    "![COCO Skeleton](images/keypoints-skeleton.png) ![COCO Examples](images/keypoints-examples.png)\n",
    "\n",
    "*Example of COCO Keypoint Annotations (Source: COCO Dataset)*\n",
    "\n",
    "Read more about COCO dataset <a href=\"https://cocodataset.org/#keypoints-2017\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed95543-736a-435e-8e32-87e427398222",
   "metadata": {},
   "source": [
    "### 2.1.1 Get data using fiftyone library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc4fb73-1189-4304-8ed0-0c31ff32d748",
   "metadata": {},
   "source": [
    "[FiftyOne](https://voxel51.com/fiftyone/) is an open-source toolset by Voxel51 designed for building high-quality datasets and computer vision models. It acts as a visual interface for your datasets, allowing you to:\n",
    "\n",
    "- **Visualize** raw images and their annotations (bounding boxes, keypoints, etc.) instantly.\n",
    "- **Query** specific samples (e.g., \"show me all images with > 10 people\").\n",
    "- **Debug** model predictions by overlaying them on ground truth.\n",
    "- **Manage** data splits and export to various formats (COCO, YOLO, TFRecord)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35f3f3ef-3e88-4369-8cf7-98c591739ac2",
   "metadata": {
    "id": "35f3f3ef-3e88-4369-8cf7-98c591739ac2",
    "outputId": "f22f7827-0ae6-497b-b83f-76b1b58372a8"
   },
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b855961-7fae-4161-b88e-25dcddc26301",
   "metadata": {
    "id": "9b855961-7fae-4161-b88e-25dcddc26301",
    "outputId": "9e82d832-1bad-4b63-fd76-8cfc2f7c414f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'train' to '/home/clinton-mwangi/fiftyone/coco-2017/train' if necessary\n",
      "Found annotations at '/home/clinton-mwangi/fiftyone/coco-2017/raw/instances_train2017.json'\n",
      "Sufficient images already downloaded\n",
      "Existing download of split 'train' is sufficient\n",
      "Loading existing dataset 'coco-2017-train-2000'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    }
   ],
   "source": [
    "train_data = foz.load_zoo_dataset(\"coco-2017\", split='train',max_samples=2000,  label_types=[\"keypoints\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc73542c-6b56-4aec-ac06-2faeddb3c058",
   "metadata": {
    "id": "dc73542c-6b56-4aec-ac06-2faeddb3c058",
    "outputId": "fbbf127a-6813-4dbd-9915-2fd02fba6b15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'validation' to '/home/clinton-mwangi/fiftyone/coco-2017/validation' if necessary\n",
      "Found annotations at '/home/clinton-mwangi/fiftyone/coco-2017/raw/instances_val2017.json'\n",
      "Sufficient images already downloaded\n",
      "Existing download of split 'validation' is sufficient\n",
      "Loading existing dataset 'coco-2017-validation-1000'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    }
   ],
   "source": [
    "val_data = foz.load_zoo_dataset(\"coco-2017\", split=\"validation\",  max_samples=1000, label_types=[\"keypoints\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e44a357-7a11-4071-a95d-88821bcfa4a8",
   "metadata": {
    "id": "2e44a357-7a11-4071-a95d-88821bcfa4a8",
    "outputId": "4cce8813-f18e-4753-96a9-2d4db191daae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name:        coco-2017-train-2000\n",
       "Media type:  None\n",
       "Num samples: 0\n",
       "Persistent:  False\n",
       "Tags:        []\n",
       "Sample fields:\n",
       "    id:               fiftyone.core.fields.ObjectIdField\n",
       "    filepath:         fiftyone.core.fields.StringField\n",
       "    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n",
       "    created_at:       fiftyone.core.fields.DateTimeField\n",
       "    last_modified_at: fiftyone.core.fields.DateTimeField"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd79bb20-0339-452d-a5b1-3cccd18ce6cf",
   "metadata": {
    "id": "bd79bb20-0339-452d-a5b1-3cccd18ce6cf",
    "outputId": "9b466a28-d275-448a-edf6-2573908bfe9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session launched. Run `session.show()` to open the App in a cell output.\n"
     ]
    }
   ],
   "source": [
    "session = fo.launch_app(val_data, auto=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9077f055-d120-43ee-9dab-28bc2d254225",
   "metadata": {
    "id": "045c85fd-69ba-4152-b87b-c54b96c559cb",
    "outputId": "8721e225-5afa-4c8a-e59f-76718515bbe7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?notebook=True&subscription=78f73f28-f702-4df4-98ad-e0d249303de6\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7a82a24486b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "session.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177333ad-6c81-4fcb-bf15-c7e5272240c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.1.2. Get data by direct download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ae12c2-ea69-4587-9389-3f57af6dd28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to store the data\n",
    "DATA_DIR=\"/path/to/store/data\"\n",
    "\n",
    "!mkdir -p $DATA_DIR\n",
    "\n",
    "!wget -c http://images.cocodataset.org/annotations/annotations_trainval2017.zip -P $DATA_DIR\n",
    "!wget -c http://images.cocodataset.org/zips/val2017.zip -P $DATA_DIR\n",
    "!wget -c http://images.cocodataset.org/zips/train2017.zip -P $DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc524cf-358a-453b-bb1b-34f21c469a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the files\n",
    "!unzip -q $DATA_DIR/annotations_trainval2017.zip -d $DATA_DIR\n",
    "!unzip -q $DATA_DIR/val2017.zip -d $DATA_DIR\n",
    "!unzip -q $DATA_DIR/train2017.zip -d $DATA_DIR\n",
    "\n",
    "# Cleanup zip files\n",
    "!rm -rf $DATA_DIR/*.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeaef20-70c6-4bb2-a955-1cad726b23ca",
   "metadata": {
    "id": "dbeaef20-70c6-4bb2-a955-1cad726b23ca"
   },
   "source": [
    "## 2.2 Data Prepocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9d733d-8a11-4f40-95ae-0d14343312be",
   "metadata": {
    "id": "ee9d733d-8a11-4f40-95ae-0d14343312be"
   },
   "source": [
    "We will follow the following data preprocessing that are unique to human pose estimation tasks based on paper: <a href=\"https://arxiv.org/abs/1911.07524\"> The devils is in the details </a>.\n",
    "1.  **Filtering**: We first check if an image actually contains a person with valid annotations. \n",
    "2.  **Cropping**: Once we identify a valid image, we locate the person using their bounding box and **crop** the image to center on them. \n",
    "3.  **Coordinate Transformation**: We transform the original keypoint annotations from the full image space into our new **cropped coordinate space**.\n",
    "4.  **Ground Truth Generation**: Finally, we take these transformed keypoints and generate **Gaussian Heatmaps**, which serve as the training targets for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7aa3f3b-708f-45ae-8c1f-3056e1937957",
   "metadata": {
    "id": "b7aa3f3b-708f-45ae-8c1f-3056e1937957"
   },
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, target_size=(288, 384), heatmap_size=(72, 96), sigma=2.0):\n",
    "        self.target_size = target_size\n",
    "        self.heatmap_size = heatmap_size\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        # Pre-compute the coordinate grid for heatmap generation\n",
    "        W, H = self.heatmap_size\n",
    "        self.yy, self.xx = np.meshgrid(np.arange(H), np.arange(W), indexing='ij')\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_keypoints_and_visibility(keypoints: List[float]):\n",
    "        try:\n",
    "            kps_array = np.array(keypoints, dtype=np.float32).reshape(-1, 3)\n",
    "            coords = kps_array[:, :2]\n",
    "            visibility = kps_array[:, 2]\n",
    "            return coords, visibility.astype(np.int32)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting keypoints: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def process_image_and_keypoints(\n",
    "        self,\n",
    "        image: np.ndarray,\n",
    "        keypoints: np.ndarray,\n",
    "        bbox: Union[List[int], Tuple[int, int, int, int]],\n",
    "        angle: float = 0,\n",
    "        flip: bool = False\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Sequential process of proocessing the input image\n",
    "        \"\"\"\n",
    "        try:\n",
    "            x1, y1, w, h = np.round(bbox).astype(int)\n",
    "            img_h, img_w = image.shape[:2]\n",
    "\n",
    "            # Step 1: Safe Crop\n",
    "            cx1, cy1 = max(0, x1), max(0, y1) #Why do we do this?\n",
    "            cx2, cy2 = min(img_w, x1 + w), min(img_h, y1 + h)\n",
    "            image = image[cy1:cy2, cx1:cx2]\n",
    "            crop_h, crop_w = image.shape[:2]\n",
    "\n",
    "            # Step 2: Prepare Keypoints\n",
    "            kps = keypoints.copy().astype(np.float32)\n",
    "            zero_mask = np.all(kps == 0, axis=1)\n",
    "            kps -= [cx1, cy1]\n",
    "\n",
    "            # Step 3: Boundary Masking (Vectorized)\n",
    "            invalid_mask = (kps[:, 0] < 0) | (kps[:, 1] < 0) | (kps[:, 0] >= crop_w) | (kps[:, 1] >= crop_h)\n",
    "\n",
    "            # Step 4: Resize (High Speed via OpenCV)\n",
    "            image = cv2.resize(image, self.target_size, interpolation=cv2.INTER_LINEAR)\n",
    "            kps *= [self.target_size[0] / crop_w, self.target_size[1] / crop_h]\n",
    "\n",
    "            # Step 5: Rotation\n",
    "            if angle != 0:\n",
    "                center = (self.target_size[0] / 2, self.target_size[1] / 2)\n",
    "                M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "                image = cv2.warpAffine(image, M, self.target_size)\n",
    "                ones = np.ones((kps.shape[0], 1))\n",
    "                kps_homo = np.hstack([kps, ones])\n",
    "                kps = kps_homo @ M.T\n",
    "\n",
    "            # Step 6: Horizontal Flip\n",
    "            if flip:\n",
    "                image = cv2.flip(image, 1)\n",
    "                kps[:, 0] = self.target_size[0] - kps[:, 0]\n",
    "\n",
    "            # Step 7: Restore invalid points to zero\n",
    "            kps[zero_mask | invalid_mask] = 0\n",
    "\n",
    "            return image, kps\n",
    "        except Exception as e:\n",
    "            print(f\"Error in processing: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def generate_heatmaps(\n",
    "        self,\n",
    "        keypoints: np.ndarray,\n",
    "        keypoints_visible: np.ndarray\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Highly optimized Gaussian Heatmap generation using pre-computed grids.\n",
    "        \"\"\"\n",
    "        W_hm, H_hm = self.heatmap_size\n",
    "        W_img, H_img = self.target_size\n",
    "        scale_x = W_hm / W_img\n",
    "        scale_y = H_hm / H_img\n",
    "        kps_hm = keypoints * [scale_x, scale_y]\n",
    "\n",
    "        mask = (keypoints_visible >= 0.5) & \\\n",
    "               (kps_hm[:, 0] >= 0) & (kps_hm[:, 0] < W_hm) & \\\n",
    "               (kps_hm[:, 1] >= 0) & (kps_hm[:, 1] < H_hm)\n",
    " \n",
    "        num_kps = kps_hm.shape[0]\n",
    "        heatmaps = np.zeros((num_kps, H_hm, W_hm), dtype=np.float32)\n",
    "        for i in range(num_kps):\n",
    "            if not mask[i]:\n",
    "                continue\n",
    "                \n",
    "            mu_x, mu_y = kps_hm[i]\n",
    "            dist_sq = (self.xx - mu_x) ** 2 + (self.yy - mu_y) ** 2\n",
    "            heatmaps[i] = np.exp(-dist_sq / (2 * self.sigma**2))\n",
    "\n",
    "        return torch.from_numpy(heatmaps), torch.from_numpy(mask.astype(np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9ae3dd-2af1-487a-b2b7-47273275328d",
   "metadata": {
    "id": "ac9ae3dd-2af1-487a-b2b7-47273275328d"
   },
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea9cdf37-efbb-4436-b711-0ddeffd4b8cf",
   "metadata": {
    "id": "ea9cdf37-efbb-4436-b711-0ddeffd4b8cf"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def get_downloaded_image_ids(root_dir: str, coco: COCO) -> List[int]:\n",
    "    on_disk = set(os.listdir(root_dir))\n",
    "    all_img_info = coco.loadImgs(coco.getImgIds())\n",
    "    existing_ids = [info['id'] for info in all_img_info if info['file_name'] in on_disk]\n",
    "    return existing_ids\n",
    "\n",
    "class CocoKeypoints(Dataset):\n",
    "    def __init__(self, root: str, annFile: str, target_size=(288, 384), heatmap_size=(72, 96), sigma=2.0) -> None:\n",
    "        self.root = root\n",
    "        self.coco = COCO(annFile)\n",
    "        self.processor = DataProcessor(\n",
    "            target_size=target_size, \n",
    "            heatmap_size=heatmap_size, \n",
    "            sigma=sigma\n",
    "        )\n",
    "        \n",
    "        on_disk = set(os.listdir(root))\n",
    "        self.samples = []\n",
    "        img_ids = self.coco.getImgIds()\n",
    "        all_imgs = self.coco.loadImgs(img_ids)\n",
    "        img_id_to_file = {img['id']: img['file_name'] for img in all_imgs if img['file_name'] in on_disk}\n",
    "\n",
    "        for img_id in img_id_to_file:\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "            for ann in anns:\n",
    "                if ann.get(\"num_keypoints\", 0) > 0:\n",
    "                    self.samples.append({\n",
    "                        \"file_name\": img_id_to_file[img_id],\n",
    "                        \"keypoints\": ann[\"keypoints\"],\n",
    "                        \"bbox\": ann[\"bbox\"]\n",
    "                    })\n",
    "\n",
    "        logger.info(f\"Initialized {len(self.samples)} samples.\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        sample = self.samples[index]\n",
    "        img_path = os.path.join(self.root, sample[\"file_name\"])\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            return torch.zeros((3, *self.processor.target_size)), torch.zeros((17, *self.processor.heatmap_size)), \n",
    "            torch.zeros(17), torch.zeros((17, 2))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        coords, visibility = self.processor.extract_keypoints_and_visibility(sample[\"keypoints\"])\n",
    "        processed_img_np, processed_kps = self.processor.process_image_and_keypoints(\n",
    "            image, \n",
    "            coords, \n",
    "            sample[\"bbox\"]\n",
    "        )\n",
    "        heatmaps, masks = self.processor.generate_heatmaps(processed_kps, visibility)\n",
    "        img_tensor = torch.from_numpy(processed_img_np.copy()).permute(2, 0, 1).float().div(255.0)\n",
    "        coords_tensors = torch.from_numpy(processed_kps.copy()) \n",
    "        return img_tensor, heatmaps, masks, coords_tensors\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "def get_coco_dataloaders(root_train, ann_train, root_val, ann_val, batch_size=32, num_workers=8):\n",
    "    train_dataset = CocoKeypoints(root_train, ann_train)\n",
    "    val_dataset = CocoKeypoints(root_val, ann_val)    \n",
    "    loader_args = dict(\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True, \n",
    "        collate_fn=collate_fn,\n",
    "        persistent_workers=True \n",
    "    )\n",
    "    return (\n",
    "        DataLoader(train_dataset, shuffle=True, **loader_args),\n",
    "        DataLoader(val_dataset, shuffle=False, **loader_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69b956cc-0790-4445-bf03-dbecea82fdea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/clinton-mwangi/fiftyone'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zoo_dir = fo.config.dataset_zoo_dir\n",
    "zoo_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ab6c16b-98fc-4d76-8341-f026eeab3390",
   "metadata": {
    "id": "6ab6c16b-98fc-4d76-8341-f026eeab3390",
    "outputId": "dedb4965-6359-4ba8-adae-76b71f91bc18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=9.33s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 17:16:07,226 - INFO - Initialized 38151 samples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 17:16:07,706 - INFO - Initialized 6352 samples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.39s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "#Fiftyone downloads the data in your home directory by default\n",
    "\n",
    "coco_root = os.path.join(fo.config.dataset_zoo_dir, \"coco-2017\")\n",
    "root_train = os.path.join(coco_root, \"train\", \"data\")\n",
    "root_val = os.path.join(coco_root, \"validation\", \"data\")\n",
    "\n",
    "ann_train = os.path.join(coco_root, \"raw\", \"person_keypoints_train2017.json\")\n",
    "ann_val = os.path.join(coco_root, \"raw\", \"person_keypoints_val2017.json\")\n",
    "\n",
    "train_loader, val_loader = get_coco_dataloaders(\n",
    "    root_train=root_train,\n",
    "    ann_train=ann_train,\n",
    "    root_val=root_val,\n",
    "    ann_val=ann_val,\n",
    "    batch_size=32,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62702d8c-33db-4d88-89af-806cde700003",
   "metadata": {},
   "source": [
    "# 3. Training and distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586def00-6565-4c0c-8d7f-5e84a62a0b99",
   "metadata": {},
   "source": [
    "## 3.1 Baseline Student Training\n",
    "\n",
    "We start by training the **Student Model (SqueezeNet)** purely on the Ground Truth labels, **without any distillation**. on the sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b606189-a609-4d68-858f-b9a81cbd3c10",
   "metadata": {
    "id": "5b606189-a609-4d68-858f-b9a81cbd3c10"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "student_model = SqueezeNetHPE(num_keypoints=17).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f0f05d-dd05-4e92-81ee-0357e81e3323",
   "metadata": {},
   "source": [
    "### Training class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f342e40-6dae-4eaa-bb60-f71401044a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseDistillationTrainer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        student_model, \n",
    "        device, \n",
    "        teacher_model=None,\n",
    "        mode='baseline', \n",
    "        checkpoint_dir='checkpoints',\n",
    "        lr=1e-3,\n",
    "        num_epochs=10,\n",
    "        temp=4.0,\n",
    "        alpha_logit=1.0,\n",
    "        alpha_feat=0.5,\n",
    "        student_layer=None,\n",
    "        teacher_layer=None,\n",
    "        s_channels=None,\n",
    "        t_channels=None\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.mode = mode\n",
    "        self.student = student_model.to(device)\n",
    "        self.teacher = teacher_model.to(device).eval() if teacher_model else None\n",
    "        self.num_epochs = num_epochs\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        \n",
    "        # KD Parameters\n",
    "        self.temp = temp\n",
    "        self.alpha_logit = alpha_logit\n",
    "        self.alpha_feat = alpha_feat\n",
    "        self.best_val_loss = float('inf')\n",
    "\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "        # 1. Setup Feature Distillation (Hooks & Adapters)\n",
    "        self.adapter = None\n",
    "        if 'feature' in mode or 'full' in mode:\n",
    "            self.t_features, self.s_features = {}, {}\n",
    "            self._register_hooks(student_layer, teacher_layer)\n",
    "            self.adapter = nn.Conv2d(s_channels, t_channels, 1).to(device)\n",
    "\n",
    "        # 2. Optimization Setup\n",
    "        self.criterion = AdaptiveWingLoss().to(device)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        \n",
    "        train_params = list(self.student.parameters())\n",
    "        if self.adapter:\n",
    "            train_params += list(self.adapter.parameters())\n",
    "            \n",
    "        self.optimizer = optim.AdamW(train_params, lr=lr, weight_decay=1e-4)\n",
    "        \n",
    "        # Consistent Scheduler: 5 epoch warmup + Cosine Decay\n",
    "        lambda_lr = lambda e: (e + 1) / 5 if e < 5 else 0.5 * (1 + math.cos(math.pi * (e - 5) / (num_epochs - 5)))\n",
    "        self.scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=lambda_lr)\n",
    "        self.scaler = GradScaler()\n",
    "\n",
    "    def _register_hooks(self, s_path, t_path):\n",
    "        def get_activation(name, storage):\n",
    "            def hook(m, i, o): storage[name] = o\n",
    "            return hook\n",
    "        \n",
    "        # Registering on teacher and student\n",
    "        dict(self.teacher.named_modules())[t_path].register_forward_hook(get_activation('feat', self.t_features))\n",
    "        dict(self.student.named_modules())[s_path].register_forward_hook(get_activation('feat', self.s_features))\n",
    "\n",
    "    def spatial_kl_loss(self, s_logits, t_logits):\n",
    "        B, K, H, W = s_logits.shape\n",
    "        s_prob = F.log_softmax(s_logits.view(B, K, -1) / self.temp, dim=-1)\n",
    "        t_prob = F.softmax(t_logits.view(B, K, -1) / self.temp, dim=-1)\n",
    "        return F.kl_div(s_prob, t_prob, reduction='batchmean') * (self.temp**2)\n",
    "\n",
    "    def train(self, train_loader, val_loader):\n",
    "        print(f\"--- Starting {self.mode.upper()} Training ---\")\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.student.train()\n",
    "            if self.adapter: self.adapter.train()\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{self.num_epochs}')\n",
    "            \n",
    "            for imgs, targets, masks, _ in pbar:\n",
    "                imgs, targets = imgs.to(self.device), targets.to(self.device)\n",
    "                masks_exp = masks.to(self.device).unsqueeze(-1).unsqueeze(-1)\n",
    "                \n",
    "                self.optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                with autocast():\n",
    "                    # Student Forward\n",
    "                    s_out = self.student(imgs)\n",
    "                    s_hms = s_out.heatmaps if hasattr(s_out, 'heatmaps') else s_out\n",
    "                    \n",
    "                    # 1. Base Loss\n",
    "                    loss = self.criterion(s_hms * masks_exp, targets * masks_exp)\n",
    "\n",
    "                    # 2. Knowledge Distillation Logic\n",
    "                    if self.mode != 'baseline':\n",
    "                        with torch.no_grad():\n",
    "                            t_out = self.teacher(imgs)\n",
    "                            t_hms = t_out.heatmaps if hasattr(t_out, 'heatmaps') else t_out\n",
    "\n",
    "                        # Logit-based KD\n",
    "                        if self.mode in ['logits', 'full']:\n",
    "                            loss += self.alpha_logit * self.spatial_kl_loss(s_hms, t_hms)\n",
    "                        \n",
    "                        # Feature-based KD\n",
    "                        if self.mode in ['feature', 'full']:\n",
    "                            s_f, t_f = self.s_features['feat'], self.t_features['feat']\n",
    "                            if s_f.shape[2:] != t_f.shape[2:]:\n",
    "                                s_f = F.interpolate(s_f, size=t_f.shape[2:], mode='bilinear')\n",
    "                            loss += self.alpha_feat * self.mse_loss(self.adapter(s_f), t_f)\n",
    "\n",
    "                self.scaler.scale(loss).backward()\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "            self.scheduler.step()\n",
    "            val_loss = self.validate(val_loader)\n",
    "            print(f\"Epoch {epoch+1} | Val Loss: {val_loss:.6f}\")\n",
    "            \n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                torch.save(self.student.state_dict(), os.path.join(self.checkpoint_dir, f'best_{self.mode}.pth'))\n",
    "\n",
    "    def validate(self, val_loader):\n",
    "        self.student.eval()\n",
    "        total_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for imgs, targets, _, _ in val_loader:\n",
    "                imgs, targets = imgs.to(self.device), targets.to(self.device)\n",
    "                outputs = self.student(imgs)\n",
    "                s_hms = outputs.heatmaps if hasattr(outputs, 'heatmaps') else outputs\n",
    "                total_loss += self.criterion(s_hms, targets).item()\n",
    "        return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5500e844-bba9-477c-8636-842aee31c924",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PoseDistillationTrainer(\n",
    "    student_model=student_model, \n",
    "    device=device, \n",
    "    mode='baseline', \n",
    "    checkpoint_dir= \"checkpoint_logits\",\n",
    "    num_epochs=10\n",
    ")\n",
    "trainer.train(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf398ddc-f226-4f0d-ba48-7ff0eff02476",
   "metadata": {},
   "source": [
    "## 3.2. Logits-based Knowledge Distillation\n",
    "\n",
    "The distillation loss is defined as the Kullback-Leibler (KL) Divergence between the softened student logits $z_s$ and teacher logits $z_t$, scaled by temperature $T^2$:\n",
    "\n",
    "$$ L_{KD} = T^2 \\cdot KL\\left( \\sigma\\left(\\frac{z_s}{T}\\right), \\sigma\\left(\\frac{z_t}{T}\\right) \\right) $$\n",
    "\n",
    "Where $\\sigma$ is the softmax function.\n",
    "\n",
    "**In Human Pose Estimation (Regression/Heatmaps):**\n",
    "Typically, MSE is used for heatmaps. However, we can treat the heatmaps as **Spatial Probability Distributions** by flattening the spatial dimensions ($H \\times W$) and applying Softmax. This allows us to use Hinton's formulation directly by softening the peaky heatmap distributions.\n",
    "\n",
    "**Loss Function:**\n",
    "$$ L_{total} = L_{GT} + \\alpha \\cdot T^2 \\cdot KL(\\text{SpatialSoftmax}(H_S/T), \\text{SpatialSoftmax}(H_T/T)) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdf6d52-ca1a-40e5-8c4e-c0ab9b52f949",
   "metadata": {},
   "source": [
    "#### Understanding Temperature ($T$) in Logits Loss\n",
    "\n",
    "You might wonder: **Why do we divide logits by a Temperature $T$?**\n",
    "\n",
    "The standard Softmax function is defined as:\n",
    "$$ \\sigma(z_i) = \\frac{\\exp(z_i)}{\\sum_j \\exp(z_j)} $$\n",
    "\n",
    "When a model is very confident, one value in inputs $z$ is much larger than the others. This makes the output probability distribution **extremely sharp** (almost 1.0 for the peak, and 0.0 for everything else).\n",
    "\n",
    "**The Problem:** If the Teacher predicts a perfect 1.0 for the correct keypoint and 0.0 everywhere else, it provides **no more information** than the Ground Truth labels! We lose the rich structural knowledge (e.g., \"this pixel is 0.001 likely, but that pixel is 0.05 likely\").\n",
    "\n",
    "**The Solution (Temperature):**\n",
    "By introducing $T > 1$, we \"soften\" the distribution:\n",
    "$$ q_i = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)} $$\n",
    "\n",
    "- **High T (> 1)**: Flattens the peaks. The Teacher's output becomes softer, spreading probability mass to neighboring pixels. This reveals the **\"Dark Knowledge\"**â€”the relationships between the peak and its surrounding area.\n",
    "- **Low T (1)**: Standard sharp Softmax.\n",
    "\n",
    "We implement this in `spatial_kl_loss`. We take the raw heatmap logits (before softmax), divide them by `temp`, and *then* apply Softmax. This forces the Student to learn not just the peak location, but the entire spatial uncertainty shape of the Teacher.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6beadef-0f70-4878-8ef1-a6e80634e1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PoseDistillationTrainer(\n",
    "    student_model=student_model, \n",
    "    teacher_model=teacher_model, \n",
    "    device=device, \n",
    "    mode='logits', \n",
    "    temp=4.0, \n",
    "    alpha_logit=1.0\n",
    ")\n",
    "trainer.train(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdd18e3-8b94-4dd7-8959-dea428c670b3",
   "metadata": {},
   "source": [
    "## 3.3 Feature-based Knowledge Distillation\n",
    "\n",
    "Feature-based KD encourages the Student to learn intermediate representations that resemble the Teacher's. Since Student and Teacher features often have different dimensions (channels/resolution), we typically use a **Connector** (e.g., 1x1 Conv) to map Student features to the Teacher's space.\n",
    "\n",
    "We will use **Forward Hooks** to extract intermediate feature maps.\n",
    "\n",
    "**Loss Function:**\n",
    "$$ L_{total} = L_{GT} + \\beta L_{Feat}(F_{Adaptor}(F_S), F_T) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068e4124-49da-4d86-83de-fd1eb9d76fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PoseDistillationTrainer(\n",
    "    student_model=student_model, \n",
    "    teacher_model=teacher_model, \n",
    "    device=device, \n",
    "    mode='full', \n",
    "    # Feature specific\n",
    "    student_layer=\"backbone.10\", \n",
    "    teacher_layer=\"layer1\",\n",
    "    s_channels=512, \n",
    "    t_channels=48,\n",
    "    # KD weights\n",
    "    alpha_logit=1.0,\n",
    "    alpha_feat=0.5\n",
    ")\n",
    "trainer.train(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3071fc45-e417-47b5-8f3e-2e4a4c9a28a6",
   "metadata": {},
   "source": [
    "# Conclusion Results Comparison\n",
    "### Evaluate Baseline Student\n",
    "We evaluate the trained student model in two ways:\n",
    "\n",
    "1.  **Qualitative Accuracy (Visual)** (Using FiftyOne to visualize model predictions vs Ground Truth.)\n",
    "2.  **Model Complexity** (FLOPs and Parameters.)\n",
    "3.  **Coco performance metrics** - Average Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee384f5-4b18-49bf-bb66-54187867a172",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self, device: torch.device):\n",
    "        self.device = device\n",
    "\n",
    "    def count_trainable_parameters(self, model: nn.Module, model_name: str = \"Model\") -> int:\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        total_size_mb = (sum(p.numel() for p in model.parameters()) * 4) / (1024**2)\n",
    "        print(f\"--- {model_name} Profile ---\")\n",
    "        print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "        print(f\"Model Size: {total_size_mb:.2f} MB\\n\")\n",
    "        return trainable_params\n",
    "\n",
    "    def compute_model_complexity(self, model: nn.Module, input_shape: Tuple[int, int, int], name_of_model: str = \"Model\"):\n",
    "        try:\n",
    "            from ptflops import get_model_complexity_info\n",
    "            silent_buffer = io.StringIO()\n",
    "            model.eval()\n",
    "            macs, _ = get_model_complexity_info(model, input_shape, as_strings=False, \n",
    "                                               print_per_layer_stat=False, verbose=False, ost=silent_buffer)\n",
    "            results = {\"GMACs\": macs / 1e9, \"GFLOPs\": (2 * macs) / 1e9}\n",
    "            print(f\"--- {name_of_model} Complexity ---\")\n",
    "            print(f\"MACs: {results['GMACs']:.3f} G | FLOPs: {results['GFLOPs']:.3f} G\\n\")\n",
    "            return results\n",
    "        except ImportError:\n",
    "            return None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def add_predictions_to_fiftyone(self, dataset, model, num_samples=20, batch_size=8, \n",
    "                                   input_size=(288, 384), heatmap_size=(72, 96), field_name=\"predictions\"):\n",
    "        model.eval().to(self.device)\n",
    "        view = dataset.take(num_samples)\n",
    "        samples_list = list(view)\n",
    "        for i in tqdm(range(0, num_samples, batch_size), desc=f\"Predicting {field_name}\"):\n",
    "            batch_samples = samples_list[i : i + batch_size]\n",
    "            batch_imgs = [cv2.resize(cv2.cvtColor(cv2.imread(s.filepath), cv2.COLOR_BGR2RGB), input_size) for s in batch_samples]\n",
    "            inputs = torch.from_numpy(np.stack(batch_imgs)).permute(0, 3, 1, 2).float().to(self.device) / 255.0\n",
    "            outputs = model(inputs)\n",
    "            hms = outputs.heatmaps if hasattr(outputs, \"heatmaps\") else outputs\n",
    "            preds, maxvals = self._get_max_preds_vectorized(hms.cpu().numpy())\n",
    "            for j, sample in enumerate(batch_samples):\n",
    "                norm_points = (preds[j] / [heatmap_size[0], heatmap_size[1]]).clip(0, 1).tolist()\n",
    "                sample[field_name] = fo.Keypoints(keypoints=[fo.Keypoint(points=norm_points, confidence=maxvals[j].flatten().tolist())])\n",
    "                sample.save()\n",
    "        return view\n",
    "\n",
    "    def _get_max_preds_vectorized(self, batch_heatmaps):\n",
    "        B, K, H, W = batch_heatmaps.shape\n",
    "        heatmaps_reshaped = batch_heatmaps.reshape((B, K, -1))\n",
    "        idx = np.argmax(heatmaps_reshaped, axis=2).reshape((B, K, 1))\n",
    "        maxvals = np.amax(heatmaps_reshaped, axis=2).reshape((B, K, 1))\n",
    "        preds = np.zeros((B, K, 2), dtype=np.float32)\n",
    "        preds[:, :, 0], preds[:, :, 1] = idx[:, :, 0] % W, idx[:, :, 0] // W\n",
    "        return preds, maxvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373d7ff4-b109-42bf-8aa9-5768c0b171dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Adding skeleton\n",
    "skeleton = fo.KeypointSkeleton(\n",
    "    labels=[\n",
    "        \"nose\", \"l_eye\", \"r_eye\", \"l_ear\", \"r_ear\", \n",
    "        \"l_shoulder\", \"r_shoulder\", \"l_elbow\", \"r_elbow\", \n",
    "        \"l_wrist\", \"r_wrist\", \"l_hip\", \"r_hip\", \n",
    "        \"l_knee\", \"r_knee\", \"l_ankle\", \"r_ankle\"\n",
    "    ],\n",
    "    edges=[\n",
    "        [0, 1], [0, 2], [1, 3], [2, 4], # Head\n",
    "        [5, 6], [5, 7], [7, 9], [6, 8], [8, 10], # Arms\n",
    "        [5, 11], [6, 12], [11, 12], # Torso\n",
    "        [11, 13], [13, 15], [12, 14], [14, 16] # Legs\n",
    "    ]\n",
    ")\n",
    "val_data.default_skeleton = skeleton\n",
    "val_data.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554ba013-c39d-493c-a4ea-932ebd03dfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ModelEvaluator(device=device)\n",
    "\n",
    "# --- 1. Teacher Evaluation ---\n",
    "evaluator.count_trainable_parameters(teacher_model, \"Teacher (HRNet)\")\n",
    "evaluator.compute_model_complexity(teacher_model, (3, 288, 384))\n",
    "evaluator.add_predictions_to_fiftyone(val_data, teacher_model, field_name=\"teacher_preds\")\n",
    "\n",
    "# --- 2. Logit-KD Student Evaluation ---\n",
    "# Load the best weights from your checkpoints_logits_kd directory\n",
    "student_model_logits.load_state_dict(torch.load('checkpoints_logits_kd/best_model.pth'))\n",
    "evaluator.count_trainable_parameters(student_model_logits, \"Student (Logit-KD)\")\n",
    "evaluator.add_predictions_to_fiftyone(val_data, student_model_logits, field_name=\"student_logit_kd\")\n",
    "\n",
    "# --- 3. Feature-KD Student Evaluation ---\n",
    "# Load the best weights from your checkpoints_feat_kd directory\n",
    "student_model_feat.load_state_dict(torch.load('checkpoints_feat_kd/best_feat_model.pth'))\n",
    "evaluator.count_trainable_parameters(student_model_feat, \"Student (Feature-KD)\")\n",
    "evaluator.add_predictions_to_fiftyone(val_data, student_model_feat, field_name=\"student_feat_kd\")\n",
    "\n",
    "# --- 4. Launch Visualization ---\n",
    "session = fo.launch_app(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417a5cb0-78f0-4c3c-a840-1af738f8e6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python(KD-venv)",
   "language": "python",
   "name": "kd_hpe_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
